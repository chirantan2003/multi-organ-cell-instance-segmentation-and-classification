{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Organ Cell Instance Segmentation and Classification (Hover-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6L0pjtDDKj4U"
   },
   "source": [
    "## 1. Setup: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdF2Xn6bKj4V"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import xml.etree.ElementTree as ET  \n",
    "\n",
    "import tifffile\n",
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.ndimage import distance_transform_edt, label, center_of_mass, find_objects\n",
    "from scipy.stats import mode\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from skimage.color import rgb2hed, hed2rgb\n",
    "import random\n",
    "import warnings\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "import time\n",
    "import timm \n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting xml to npz for faster processing. This ensures that GPU is used efficiently during training and resources are not wasted in storing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "7d5e838dcb174e5184bef0240956414a",
      "fd597347de5a4cac94de93ccd228dd78",
      "94dcefd5fe81444fa30382985a52161d",
      "b113bebd4e6a4af4a6d3bf3c2a417a47",
      "5c33a7c5a7a844e7b37bb475065243d6",
      "a666c773c138437c8b9d00fd4e6684f7",
      "54a60dc3c7dd48cb865251fc451781e9",
      "df4cfa8e3e3448bc95af367df0a2934f",
      "cfc1052c263c420b82e2c46cdcbdc303",
      "1c9af8bd349941cdaf3a2ab0bebebc9b",
      "5ab013334b2f498f82087482b1aff1cd"
     ]
    },
    "id": "Ah0FmMIwTmUZ",
    "outputId": "7cdc5237-c666-4e97-fd65-f49d5a4d0370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing...\n",
      "Image/XML Source: train\n",
      "NPZ Mask Destination: mask_new\n",
      "Found 209 XML files to process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XML Preprocessing Complete ---\n",
      "All .npz masks are saved in: mask_new\n"
     ]
    }
   ],
   "source": [
    "def preprocess_xml_to_npz(image_dir, mask_dir):\n",
    "    \"\"\"\n",
    "    One-time script to convert XML polygon annotations into .npz masks.\n",
    "\n",
    "    Reads from: image_dir (containing .tif and .xml files)\n",
    "    Saves to:   mask_dir (as .npz files)\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Starting preprocessing...\")\n",
    "    print(f\"Image/XML Source: {image_dir}\")\n",
    "    print(f\"NPZ Mask Destination: {mask_dir}\")\n",
    "\n",
    "    os.makedirs(mask_dir, exist_ok=True)\n",
    "\n",
    "    # 0 is reserved for Background\n",
    "    CLASS_NAME_TO_ID = {\n",
    "        \"Epithelial\": 1,\n",
    "        \"Lymphocyte\": 2,\n",
    "        \"Neutrophil\": 3,\n",
    "        \"Macrophage\": 4,\n",
    "    }\n",
    "\n",
    "    xml_files = sorted(glob.glob(os.path.join(image_dir, \"*.xml\")))\n",
    "    if not xml_files:\n",
    "        print(f\"Error: No .xml files found in {image_dir}. Please check your path.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(xml_files)} XML files to process.\")\n",
    "\n",
    "    for xml_path in tqdm(xml_files, desc=\"Processing XMLs\"):\n",
    "        image_id = os.path.basename(xml_path).replace(\".xml\", \"\")\n",
    "        tif_path = os.path.join(image_dir, f\"{image_id}.tif\")\n",
    "        npz_path = os.path.join(mask_dir, f\"{image_id}.npz\")\n",
    "\n",
    "        if os.path.exists(npz_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with tifffile.TiffFile(tif_path) as tif:\n",
    "                shape = tif.pages[0].shape[:2]\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: Could not read {tif_path}. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        instance_map = np.zeros(shape, dtype=np.uint16)\n",
    "        type_map = np.zeros(shape, dtype=np.uint8) # 0 is BG\n",
    "\n",
    "        current_instance_id = 1\n",
    "\n",
    "        # Parse the XML file\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            for annotation in root.findall(\"Annotation\"):\n",
    "\n",
    "                class_name_element = annotation.find(\"Attributes/Attribute\")\n",
    "                if class_name_element is None:\n",
    "                    continue\n",
    "\n",
    "                class_name = class_name_element.get(\"Name\")\n",
    "\n",
    "                if class_name not in CLASS_NAME_TO_ID:\n",
    "                    continue \n",
    "\n",
    "                class_id = CLASS_NAME_TO_ID[class_name]\n",
    "\n",
    "                for region in annotation.findall(\"Regions/Region\"):\n",
    "                    vertices = []\n",
    "                    for vertex in region.findall(\"Vertices/Vertex\"):\n",
    "                        x = round(float(vertex.get(\"X\")))\n",
    "                        y = round(float(vertex.get(\"Y\")))\n",
    "                        vertices.append([x, y])\n",
    "\n",
    "                    if not vertices:\n",
    "                        continue\n",
    "\n",
    "                    polygon = np.array(vertices, dtype=np.int32)\n",
    "\n",
    "                    cv2.fillPoly(instance_map, [polygon], current_instance_id)\n",
    "                    cv2.fillPoly(type_map, [polygon], class_id)\n",
    "\n",
    "                    current_instance_id += 1\n",
    "\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"\\nWarning: Failed to parse {xml_path}. Skipping. Error: {e}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"\\nWarning: An error occurred processing {xml_path}. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        if current_instance_id > 1: \n",
    "            np.savez_compressed(npz_path, instance_map=instance_map, type_map=type_map)\n",
    "\n",
    "    print(f\"--- XML Preprocessing Complete ---\")\n",
    "    print(f\"All .npz masks are saved in: {mask_dir}\")\n",
    "\n",
    "\n",
    "IMAGE_DIR = \"train\"\n",
    "\n",
    "NEW_MASK_DIR = \"mask_new\"\n",
    "\n",
    "preprocess_xml_to_npz(IMAGE_DIR, NEW_MASK_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTCo3nRuKj4V",
    "outputId": "5662f6f4-a9bf-414a-e3ae-98a7d89abac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data Dirs: train, mask_new, test_final\n",
      "Model will be saved to: best_model_new_3.pth\n",
      "Submission will be saved to: submission_new_2.csv\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"train\"\n",
    "MASK_DIR = \"mask_new\"                 # Directory with training .npz masks\n",
    "TEST_DIR = \"test_final\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PATCH_SIZE = 256      \n",
    "OVERLAP = 64          \n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50           # This model was earlier trained for 50 epochs and then retrained for another 50, making it 100 epochs in total\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 2       \n",
    "VALID_SPLIT = 0.2     \n",
    "MODEL_SAVE_PATH = \"best_model_new.pth\"\n",
    "SUBMISSION_PATH = \"submission_new.csv\"\n",
    "\n",
    "NUM_CLASSES = 4 \n",
    "TP_MAP_CHANNELS = NUM_CLASSES + 1 # (Background + 4 cell types)\n",
    "\n",
    "CLASS_LOSS_WEIGHTS = torch.tensor([1.0, 1.0, 10.0, 10.0]).to(DEVICE)\n",
    "\n",
    "SUBMISSION_CLASSES = ['Epithelial', 'Lymphocyte', 'Neutrophil', 'Macrophage']\n",
    "CLASS_MAP = {\n",
    "    1: 'Epithelial',\n",
    "    2: 'Lymphocyte',\n",
    "    3: 'Neutrophil',\n",
    "    4: 'Macrophage'\n",
    "}\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Data Dirs: {DATA_DIR}, {MASK_DIR}, {TEST_DIR}\")\n",
    "print(f\"Model will be saved to: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Submission will be saved to: {SUBMISSION_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RLE Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cwv0pCf3Kj4W",
    "outputId": "a0fa7e3f-6f8e-48e2-b703-ac48911c5ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Mask:\n",
      "[[0 0 1 1 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [2 2 0 0 0]\n",
      " [2 2 0 0 0]]\n",
      "\n",
      "RLE String: 2 4 2 2 9 2 1 11 2 1 16 2\n",
      "\n",
      "Decoded Mask:\n",
      "[[0 0 1 1 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [2 2 0 0 0]\n",
      " [2 2 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def rle_encode_instance_mask(mask: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Convert an instance segmentation mask (H,W) -> RLE triple string.\n",
    "    0 = background, >0 = instance IDs.\n",
    "    Note: The 'start' index is 1-based and in Fortran (column-major) order.\n",
    "    \"\"\"\n",
    "    pixels = mask.flatten(order=\"F\").astype(np.int32)\n",
    "\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "\n",
    "    rle = []\n",
    "    for i in range(0, len(runs) - 1):\n",
    "        start = runs[i]\n",
    "        end = runs[i+1]\n",
    "        length = end - start\n",
    "        val = pixels[start] \n",
    "\n",
    "        \n",
    "        if val > 0:\n",
    "            rle.extend([val, start, length])\n",
    "\n",
    "    if not rle:\n",
    "        return \"0\" \n",
    "\n",
    "    return \" \".join(map(str, rle))\n",
    "\n",
    "\n",
    "def rle_decode_instance_mask(rle: str, shape: tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert RLE triple string back into an instance mask of shape (H, W).\n",
    "    \"\"\"\n",
    "    if not rle or str(rle).strip() in (\"1\", \"0\", \"nan\"):\n",
    "        return np.zeros(shape, dtype=np.uint16)\n",
    "\n",
    "    s = list(map(int, rle.split()))\n",
    "    mask = np.zeros(shape[0] * shape[1], dtype=np.uint16)\n",
    "\n",
    "    for i in range(0, len(s), 3):\n",
    "        val, start, length = s[i], s[i+1], s[i+2]\n",
    "        # RLE start is 1-based\n",
    "        mask[start-1:start-1+length] = val\n",
    "\n",
    "    # Reshape using Fortran (column-major) order\n",
    "    return mask.reshape(shape, order=\"F\")\n",
    "\n",
    "test_mask = np.zeros((5, 5), dtype=np.uint16)\n",
    "test_mask[0:2, 2:4] = 1\n",
    "test_mask[3:5, 0:2] = 2\n",
    "\n",
    "print(\"Original Mask:\")\n",
    "print(test_mask)\n",
    "\n",
    "rle_string = rle_encode_instance_mask(test_mask)\n",
    "print(f\"\\nRLE String: {rle_string}\")\n",
    "\n",
    "decoded_mask = rle_decode_instance_mask(rle_string, (5, 5))\n",
    "print(\"\\nDecoded Mask:\")\n",
    "print(decoded_mask)\n",
    "\n",
    "assert np.array_equal(test_mask, decoded_mask), \"RLE functions are not inverses!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratified Train/Validation Split\n",
    "\n",
    "This is a critical step. We can't use a simple random split. If we do, we might end up with no rare cells (Neutrophils, Macrophages) in our validation set, giving us a misleadingly high score.\n",
    "\n",
    "We will:\n",
    "1.  Load every type_map from the MASK_DIR.\n",
    "2.  For each image, check if it contains at least one pixel of a rare class (Neutrophil or Macrophage).\n",
    "3.  Create a \"stratification key\" for each image (e.g., has_rare_cell).\n",
    "4.  Use sklearn.model_selection.train_test_split to create an 80/20 split that respects this key, ensuring both train and val sets get a similar percentage of rare-cell-containing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "2619e12dae154a3db0e3be8f58cbdf74",
      "5caf2ae04c2b4010a81d7ebbf9f33adc",
      "b7278da1bedb484eb16f0246706ba52e",
      "29e58f7ae97e4627af2418e0ab220031",
      "c599055a9c7744a59b77021ff6c13b76",
      "5dd7a84bab694679b755da384106ad4f",
      "ef60c8ebf7cb400bbfc8243c44ed385f",
      "9952bed1ecc04372b02b319f8418da41",
      "2e700ea98a1c46348f08cb4922e03c25",
      "05dabbb4c8a342e8bd9fe88526a54833",
      "4439348b875443709c26ecd53cef8089"
     ]
    },
    "id": "yoEG54WLKj4W",
    "outputId": "5ad3a474-88ca-4389-f7fd-913c29458fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating stratified train/validation split...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 209\n",
      "Training images: 167\n",
      "Validation images: 42\n",
      "Rare images in train: 106 / 167 (63.5%)\n",
      "Rare images in val: 27 / 42 (64.3%)\n",
      "Split created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_stratified_split(mask_dir, valid_split=0.2, random_state=42):\n",
    "    print(\"Creating stratified train/validation split...\")\n",
    "    image_ids = []\n",
    "    stratify_keys = []\n",
    "\n",
    "    mask_files = sorted(glob.glob(os.path.join(mask_dir, \"*.npz\")))\n",
    "    if not mask_files:\n",
    "        print(f\"Error: No .npz mask files found in {mask_dir}. Make sure your MASK_DIR is correct.\")\n",
    "        return None, None\n",
    "\n",
    "    # Our internal mapping: 3=Neutrophil, 4=Macrophage\n",
    "    rare_class_indices = [3, 4]\n",
    "\n",
    "    for mask_path in tqdm(mask_files, desc=\"Analyzing masks\"):\n",
    "        image_id = os.path.basename(mask_path).replace(\".npz\", \"\")\n",
    "        try:\n",
    "            with np.load(mask_path) as data:\n",
    "                type_map = data['type_map']\n",
    "\n",
    "            image_ids.append(image_id)\n",
    "\n",
    "            # Find all unique classes present in the mask\n",
    "            unique_classes = np.unique(type_map)\n",
    "\n",
    "            # Stratification key: 0=common only, 1=has rare cell\n",
    "            has_rare = 0\n",
    "            for rare_idx in rare_class_indices:\n",
    "                if rare_idx in unique_classes:\n",
    "                    has_rare = 1\n",
    "                    break\n",
    "            stratify_keys.append(has_rare)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load or process {mask_path}. Skipping. Error: {e}\")\n",
    "\n",
    "    if not image_ids:\n",
    "        print(\"Error: No valid masks were processed.\")\n",
    "        return None, None\n",
    "\n",
    "    # Perform the stratified split\n",
    "    train_ids, val_ids, train_keys, val_keys = train_test_split(\n",
    "        image_ids,\n",
    "        stratify_keys,\n",
    "        test_size=valid_split,\n",
    "        random_state=random_state,\n",
    "        stratify=stratify_keys\n",
    "    )\n",
    "\n",
    "    print(f\"Total images: {len(image_ids)}\")\n",
    "    print(f\"Training images: {len(train_ids)}\")\n",
    "    print(f\"Validation images: {len(val_ids)}\")\n",
    "    print(f\"Rare images in train: {np.sum(train_keys)} / {len(train_keys)} ({np.sum(train_keys)/len(train_keys)*100:.1f}%)\")\n",
    "    print(f\"Rare images in val: {np.sum(val_keys)} / {len(val_ids)} ({np.sum(val_keys)/len(val_ids)*100:.1f}%)\")\n",
    "    print(\"Split created successfully.\")\n",
    "\n",
    "    return train_ids, val_ids\n",
    "\n",
    "train_ids, val_ids = create_stratified_split(MASK_DIR, valid_split=VALID_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing (Final Version)\n",
    "\n",
    "This section defines the core PyTorch Dataset. \n",
    "Its main jobs:\n",
    "1.  Load an image and its corresponding mask from the provided ID list.\n",
    "2.  Handle data augmentation (geometric and H&E color transforms).\n",
    "3.  Use the robust **\"Pad-then-Crop\"** logic to ensure all output patches are `PATCH_SIZE x PATCH_SIZE`.\n",
    "4.  On-the-fly, generate the three target maps:\n",
    "    * `np_map` (Nucleus Pixel): Binary, 0 or 1.\n",
    "    * `hv_map` (Horizontal/Vertical): 2-channel float vector map.\n",
    "    * `tp_map` (Type Pixel): Long tensor with class indices `0` (BG), `1` (Epi), `2` (Lym), `3` (Neu), `4` (Mac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNUYkpk0Kj4X"
   },
   "outputs": [],
   "source": [
    "def get_hv_map(instance_map):\n",
    "    \"\"\"Calculates the horizontal and vertical distance maps for each instance.\"\"\"\n",
    "    H, W = instance_map.shape\n",
    "    hv_map = np.zeros((H, W, 2), dtype=np.float32)\n",
    "\n",
    "    # Find all unique instance IDs (ignore 0, which is background)\n",
    "    instance_ids = np.unique(instance_map)[1:]\n",
    "\n",
    "    for inst_id in instance_ids:\n",
    "        inst_coords = np.where(instance_map == inst_id)\n",
    "\n",
    "        if inst_coords[0].shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate the center of mass (centroid)\n",
    "        y_center, x_center = np.mean(inst_coords[0]), np.mean(inst_coords[1])\n",
    "\n",
    "        # Calculate vectors from each pixel to the centroid\n",
    "        y_coords, x_coords = inst_coords\n",
    "\n",
    "        # Horizontal component (x-axis)\n",
    "        x_vec = x_center - x_coords\n",
    "        # Vertical component (y-axis)\n",
    "        y_vec = y_center - y_coords\n",
    "\n",
    "        # Normalize vectors\n",
    "        x_vec_max = np.max(np.abs(x_vec)) + 1e-8\n",
    "        y_vec_max = np.max(np.abs(y_vec)) + 1e-8\n",
    "\n",
    "        x_vec = x_vec / x_vec_max\n",
    "        y_vec = y_vec / y_vec_max\n",
    "\n",
    "        # Assign vectors to the map\n",
    "        hv_map[y_coords, x_coords, 0] = y_vec\n",
    "        hv_map[y_coords, x_coords, 1] = x_vec\n",
    "\n",
    "    return hv_map\n",
    "\n",
    "def augment_H_and_E(img, h_scale=0.1, e_scale=0.1, h_bias=0.1, e_bias=0.1):\n",
    "    \"\"\"Performs H&E-specific color augmentation.\"\"\"\n",
    "    # Ensure image is uint8 for rgb2hed\n",
    "    if img.dtype != np.uint8:\n",
    "        img = img.astype(np.uint8)\n",
    "\n",
    "    try:\n",
    "        img_hed = rgb2hed(img)\n",
    "    except Exception as e:\n",
    "        return img\n",
    "\n",
    "    # Hematoxylin channel (nuclei)\n",
    "    h = img_hed[:, :, 0]\n",
    "    h_scale_factor = 1.0 + np.random.uniform(-h_scale, h_scale)\n",
    "    h_bias_factor = np.random.uniform(-h_bias, h_bias)\n",
    "    h = (h * h_scale_factor) + h_bias_factor\n",
    "\n",
    "    # Eosin channel (cytoplasm/stroma)\n",
    "    e = img_hed[:, :, 1]\n",
    "    e_scale_factor = 1.0 + np.random.uniform(-e_scale, e_scale)\n",
    "    e_bias_factor = np.random.uniform(-e_bias, e_bias)\n",
    "    e = (e * e_scale_factor) + e_bias_factor\n",
    "\n",
    "    # Dab channel (unused, but need to pass it back)\n",
    "    d = img_hed[:, :, 2]\n",
    "\n",
    "    img_augmented_hed = np.stack([h, e, d], axis=-1)\n",
    "    img_augmented_rgb = hed2rgb(img_augmented_hed)\n",
    "\n",
    "    # Clip and convert back to uint8\n",
    "    img_augmented_rgb = np.clip(img_augmented_rgb, 0, 1)\n",
    "    img_augmented_rgb = (img_augmented_rgb * 255).astype(np.uint8)\n",
    "\n",
    "    return img_augmented_rgb\n",
    "\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, image_ids, patch_size=256, augment=True):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.image_ids = image_ids \n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment\n",
    "\n",
    "        self.image_info = {}\n",
    "        print(f\"Initializing dataset with {len(self.image_ids)} images...\")\n",
    "        for img_id in tqdm(image_ids, desc=\"Caching image info\"):\n",
    "            img_path = os.path.join(self.image_dir, f\"{img_id}.tif\")\n",
    "            mask_path = os.path.join(self.mask_dir, f\"{img_id}.npz\")\n",
    "            if os.path.exists(img_path) and os.path.exists(mask_path):\n",
    "                self.image_info[img_id] = {\n",
    "                    \"img_path\": img_path,\n",
    "                    \"mask_path\": mask_path\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Warning: Missing file for {img_id}. Img: {os.path.exists(img_path)}, Mask: {os.path.exists(mask_path)}\")\n",
    "\n",
    "        self.image_ids = list(self.image_info.keys()) \n",
    "        print(f\"Dataset initialized. Found {len(self.image_ids)} valid image-mask pairs.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids) * 10 # 10 patches per image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx % len(self.image_ids)]\n",
    "        info = self.image_info[image_id]\n",
    "\n",
    "        try:\n",
    "            img = tifffile.imread(info[\"img_path\"])\n",
    "\n",
    "            if img.ndim == 3 and img.shape[-1] == 4:\n",
    "                img = img[:, :, :3]\n",
    "\n",
    "            if img.ndim == 2:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "            if img.dtype != np.uint8:\n",
    "                if img.max() > 255:\n",
    "                    img = (img / img.max() * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    img = img.astype(np.uint8)\n",
    "\n",
    "            with np.load(info[\"mask_path\"]) as data:\n",
    "                instance_map = data['instance_map']\n",
    "                type_map = data['type_map']\n",
    "            H, W, _ = img.shape\n",
    "\n",
    "            pad_h = max(0, self.patch_size - H)\n",
    "            pad_w = max(0, self.patch_size - W)\n",
    "\n",
    "            img_padded = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant', constant_values=0)\n",
    "            inst_padded = np.pad(instance_map, ((0, pad_h), (0, pad_w)), mode='constant', constant_values=0)\n",
    "            type_padded = np.pad(type_map, ((0, pad_h), (0, pad_w)), mode='constant', constant_values=0)\n",
    "\n",
    "            H_pad, W_pad, _ = img_padded.shape\n",
    "\n",
    "            x = np.random.randint(0, W_pad - self.patch_size + 1)\n",
    "            y = np.random.randint(0, H_pad - self.patch_size + 1)\n",
    "\n",
    "            img_patch = img_padded[y:y+self.patch_size, x:x+self.patch_size]\n",
    "            inst_patch = inst_padded[y:y+self.patch_size, x:x+self.patch_size]\n",
    "            type_patch = type_padded[y:y+self.patch_size, x:x+self.patch_size]\n",
    "\n",
    "            # Data Augmentation\n",
    "            if self.augment:\n",
    "                # Geometric\n",
    "                if random.random() > 0.5: # Horizontal Flip\n",
    "                    img_patch = np.fliplr(img_patch)\n",
    "                    inst_patch = np.fliplr(inst_patch)\n",
    "                    type_patch = np.fliplr(type_patch)\n",
    "                if random.random() > 0.5: # Vertical Flip\n",
    "                    img_patch = np.flipud(img_patch)\n",
    "                    inst_patch = np.flipud(inst_patch)\n",
    "                    type_patch = np.flipud(type_patch)\n",
    "                if random.random() > 0.5: # 90-degree Rotation\n",
    "                    k = random.choice([1, 2, 3])\n",
    "                    img_patch = np.rot90(img_patch, k)\n",
    "                    inst_patch = np.rot90(inst_patch, k)\n",
    "                    type_patch = np.rot90(type_patch, k)\n",
    "\n",
    "                # Color (H&E)\n",
    "                if random.random() > 0.5:\n",
    "                    img_patch = augment_H_and_E(img_patch,\n",
    "                                                h_scale=0.2, e_scale=0.2,\n",
    "                                                h_bias=0.2, e_bias=0.2)\n",
    "\n",
    "                # Contrast Jitter\n",
    "                if random.random() > 0.5:\n",
    "                    # 20% variation in brightness/contrast\n",
    "                    factor = np.random.uniform(0.8, 1.2)\n",
    "                    img_patch = np.clip(img_patch.astype(np.float32) * factor, 0, 255).astype(np.uint8)\n",
    "\n",
    "                # Add Gaussian Blur\n",
    "                if random.random() > 0.5:\n",
    "                    # 5x5 kernel, standard deviation 0\n",
    "                    img_patch = cv2.GaussianBlur(img_patch, (5, 5), 0)\n",
    "\n",
    "                # CoarseDropout (Cutout) to fight overfitting\n",
    "                if random.random() > 0.5:\n",
    "                    max_holes = 8\n",
    "                    max_h_size = int(self.patch_size * 0.1) # 10% of patch size\n",
    "                    max_w_size = int(self.patch_size * 0.1)\n",
    "                    for _ in range(np.random.randint(1, max_holes)):\n",
    "                        y1 = np.random.randint(0, self.patch_size - max_h_size + 1)\n",
    "                        x1 = np.random.randint(0, self.patch_size - max_w_size + 1)\n",
    "                        h = np.random.randint(1, max_h_size)\n",
    "                        w = np.random.randint(1, max_w_size)\n",
    "                        y2 = y1 + h\n",
    "                        x2 = x1 + w\n",
    "                        img_patch[y1:y2, x1:x2] = 0\n",
    "\n",
    "            # NP Map (Nucleus vs. Background)\n",
    "            np_map = (inst_patch > 0).astype(np.float32)\n",
    "\n",
    "            # HV Map (Horizontal/Vertical vectors)\n",
    "            hv_map = get_hv_map(inst_patch.copy())\n",
    "\n",
    "            # TP Map (Pixel-wise class labels)\n",
    "            tp_map = (type_patch * np_map).astype(np.int64)\n",
    "\n",
    "            img_tensor = torch.from_numpy(img_patch.copy().transpose(2, 0, 1)).float() / 255.0\n",
    "            np_tensor = torch.from_numpy(np_map).float().unsqueeze(0) \n",
    "            hv_tensor = torch.from_numpy(hv_map.transpose(2, 0, 1)).float()\n",
    "            tp_tensor = torch.from_numpy(tp_map).long() \n",
    "\n",
    "            return img_tensor, {\"np\": np_tensor, \"hv\": hv_tensor, \"tp\": tp_tensor}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading item for {image_id}. Error: {e}. Returning None.\")\n",
    "            return None\n",
    "\n",
    "def collate_fn_skip_errors(batch):\n",
    "    \"\"\"A collate_fn that filters out None values from a batch.\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if not batch:\n",
    "        return None, None\n",
    "\n",
    "    return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8Tv8yq-Kj4Y"
   },
   "source": [
    "## 5. Model Definition (Hover-Net)\n",
    "\n",
    "We will define the Hover-Net architecture. It consists of:\n",
    "1.  A pre-trained **ConvNeXt-Tiny** backbone as an encoder.\n",
    "2.  A **Feature Pyramid Network (FPN)** decoder to create a rich, multi-scale feature map.\n",
    "3.  Three separate prediction heads (small conv blocks) branching from the decoder for the `NP`, `HV`, and `TP` maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pymV8Kc2Kj4Y",
    "outputId": "cd2e3837-73f4-4e60-9a10-9df846e06c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ImageNet-pre-trained ConvNeXt-Tiny backbone from timm...\n",
      "ConvNeXt-Tiny backbone loaded successfully.\n",
      "Model instantiated successfully.\n",
      "TP Head output channels: 5\n",
      "Output shapes:\n",
      "  NP: torch.Size([2, 1, 256, 256])\n",
      "  HV: torch.Size([2, 2, 256, 256])\n",
      "  TP: torch.Size([2, 5, 256, 256])\n",
      "Model test forward pass successful.\n"
     ]
    }
   ],
   "source": [
    "class HoverNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        print(\"Loading ImageNet-pre-trained ConvNeXt-Tiny backbone from timm...\")\n",
    "        self.backbone = timm.create_model(\n",
    "            'convnext_tiny',\n",
    "            pretrained=True,\n",
    "            features_only=True\n",
    "        )\n",
    "        print(\"ConvNeXt-Tiny backbone loaded successfully.\")\n",
    "        \n",
    "        # Channel sizes for convnext_tiny \n",
    "        # c2: /4 scale,  96 channels\n",
    "        # c3: /8 scale, 192 channels\n",
    "        # c4: /16 scale, 384 channels\n",
    "        # c5: /32 scale, 768 channels\n",
    "        \n",
    "        # Decoder (FPN-like)\n",
    "        self.lateral_c5 = nn.Conv2d(768, 256, 1)\n",
    "        self.lateral_c4 = nn.Conv2d(384, 256, 1)\n",
    "        self.lateral_c3 = nn.Conv2d(192, 256, 1)\n",
    "        self.lateral_c2 = nn.Conv2d(96,  256, 1)\n",
    "\n",
    "        # Top-down upsampling layers\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Smoothing (3x3 convs)\n",
    "        self.smooth_p4 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.smooth_p3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.smooth_p2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "        # Final Decoder Block \n",
    "        self.final_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.final_conv1 = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.final_relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.final_upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.final_conv2 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.final_relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 1. Nucleus Pixel (NP) Head\n",
    "        self.head_np = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(64, 1, 1) \n",
    "        )\n",
    "\n",
    "        # 2. Horizontal-Vertical (HV) Head\n",
    "        self.head_hv = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(64, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        # 3. Nucleus Type (TP) Head\n",
    "        self.head_tp = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(64, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        c2, c3, c4, c5 = self.backbone(x)\n",
    "\n",
    "        # Decoder (FPN)\n",
    "        p5 = self.lateral_c5(c5)\n",
    "\n",
    "        p4 = self.upsample(p5) + self.lateral_c4(c4)\n",
    "        p4 = self.smooth_p4(p4)\n",
    "\n",
    "        p3 = self.upsample(p4) + self.lateral_c3(c3)\n",
    "        p3 = self.smooth_p3(p3)\n",
    "\n",
    "        p2 = self.upsample(p3) + self.lateral_c2(c2)\n",
    "        p2 = self.smooth_p2(p2)\n",
    "\n",
    "        # Final Decoder Block\n",
    "        d1 = self.final_upsample1(p2)\n",
    "        d1 = self.final_relu1(self.final_conv1(d1))\n",
    "\n",
    "        d2 = self.final_upsample2(d1)\n",
    "        d2 = self.final_relu2(self.final_conv2(d2))\n",
    "\n",
    "        # Heads\n",
    "        out_np = self.head_np(d2) \n",
    "        out_hv = self.head_hv(d2)\n",
    "        out_tp = self.head_tp(d2)\n",
    "\n",
    "        return {\"np\": out_np, \"hv\": out_hv, \"tp\": out_tp}\n",
    "\n",
    "\n",
    "try:\n",
    "    if 'TP_MAP_CHANNELS' not in locals():\n",
    "         TP_MAP_CHANNELS = 5 # BG + 4 classes\n",
    "    if 'DEVICE' not in locals():\n",
    "        DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if 'PATCH_SIZE' not in locals():\n",
    "        PATCH_SIZE = 256\n",
    "\n",
    "    model = HoverNet(num_classes=TP_MAP_CHANNELS).to(DEVICE)\n",
    "    print(\"Model instantiated successfully.\")\n",
    "    print(f\"TP Head output channels: {TP_MAP_CHANNELS}\")\n",
    "\n",
    "    dummy_input = torch.randn(2, 3, PATCH_SIZE, PATCH_SIZE).to(DEVICE)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"Output shapes:\")\n",
    "    print(f\"  NP: {output['np'].shape}\")\n",
    "    print(f\"  HV: {output['hv'].shape}\")\n",
    "    print(f\"  TP: {output['tp'].shape}\")\n",
    "    assert output['np'].shape == (2, 1, PATCH_SIZE, PATCH_SIZE)\n",
    "    assert output['hv'].shape == (2, 2, PATCH_SIZE, PATCH_SIZE)\n",
    "    assert output['tp'].shape == (2, TP_MAP_CHANNELS, PATCH_SIZE, PATCH_SIZE)\n",
    "    print(\"Model test forward pass successful.\")\n",
    "    del model, dummy_input, output # Free memory\n",
    "except Exception as e:\n",
    "    print(f\"Error during model test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqy5eGYvKj4Y"
   },
   "source": [
    "## 6. Loss Functions\n",
    "\n",
    "We need a combined loss function. We will use:\n",
    "1.  **BCEWithLogitsLoss** for the `NP` (binary segmentation) head.\n",
    "2.  **Mean Squared Error (MSE)** for the `HV` (regression) head. We only calculate this loss on pixels that are *inside* a nucleus (i.e., where `np_true == 1`).\n",
    "3.  **CrossEntropyLoss** for the `TP` (classification) head.\n",
    "\n",
    "**CRITICAL:** To handle the class imbalance, we will use a **Weighted Cross Entropy Loss** (or Focal Loss). We will provide our `CLASS_LOSS_WEIGHTS` `[1, 1, 10, 10]` to the `TP` loss function and tell it to `ignore_index=0` (the background class).\n",
    "\n",
    "Let's implement **Focal Loss**, as it's generally superior to simple weighted CE for extreme imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36SQDjMgKj4Y",
    "outputId": "c16a56d3-809d-4800-bf84-d12563809ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function instantiated successfully.\n",
      "Loss calculation successful:\n",
      "  Total: 18.2983\n",
      "  NP Loss: 0.8088\n",
      "  HV Loss: 4.0461\n",
      "  TP Loss: 8.5886\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for multi-class classification.\n",
    "    Assumes logits as input and class indices as target.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean', ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha \n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs shape: (B, C, H, W)\n",
    "        # targets shape: (B, H, W)\n",
    "\n",
    "        # Calculate CrossEntropyLoss (but keep it per-pixel)\n",
    "        # This is the negative log-likelihood\n",
    "        ce_loss = F.cross_entropy(inputs, targets,\n",
    "                                  weight=self.alpha,\n",
    "                                  ignore_index=self.ignore_index,\n",
    "                                  reduction='none')\n",
    "\n",
    "        # Get the probabilities of the correct class\n",
    "        pt = torch.exp(-ce_loss)\n",
    "\n",
    "        # Calculate Focal Loss\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "class CombinedHoverLoss(nn.Module):\n",
    "    def __init__(self, tp_class_weights=None, ignore_index=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_np = nn.BCEWithLogitsLoss()\n",
    "        self.loss_hv = nn.MSELoss(reduction='none') \n",
    "\n",
    "        if tp_class_weights is not None:\n",
    "            full_weights = torch.cat([torch.tensor([0.0]).to(DEVICE), tp_class_weights])\n",
    "        else:\n",
    "            full_weights = None\n",
    "\n",
    "        self.loss_tp = FocalLoss(alpha=full_weights, gamma=1.5, ignore_index=ignore_index)\n",
    "\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        pred_np = preds['np'] # (B, 1, H, W)\n",
    "        pred_hv = preds['hv'] # (B, 2, H, W)\n",
    "        pred_tp = preds['tp'] # (B, 5, H, W)\n",
    "\n",
    "        true_np = targets['np'] # (B, 1, H, W)\n",
    "        true_hv = targets['hv'] # (B, 2, H, W)\n",
    "        true_tp = targets['tp'] # (B, H, W) - class indices [0-4]\n",
    "\n",
    "        # NP Loss (Binary Segmentation)\n",
    "        loss_np = self.loss_np(pred_np, true_np)\n",
    "\n",
    "        # HV Loss (Regression)\n",
    "        # Get per-pixel MSE loss\n",
    "        loss_hv_per_pixel = self.loss_hv(pred_hv, true_hv) \n",
    "\n",
    "        # true_np is (B, 1, H, W), we need (B, 2, H, W) or (B, 1, H, W) for broadcasting\n",
    "        masked_loss_hv = loss_hv_per_pixel * true_np # (B, 2, H, W)\n",
    "\n",
    "        # Sum of loss / sum of non-zero pixels\n",
    "        loss_hv = masked_loss_hv.sum() / (true_np.sum() + 1e-8)\n",
    "\n",
    "        loss_tp = self.loss_tp(pred_tp, true_tp)\n",
    "\n",
    "        total_loss = 2.0 * loss_np + 2.0 * loss_hv + 1.0 * loss_tp\n",
    "\n",
    "        return total_loss, {\"np\": loss_np, \"hv\": loss_hv, \"tp\": loss_tp}\n",
    "\n",
    "try:\n",
    "    criterion = CombinedHoverLoss(tp_class_weights=CLASS_LOSS_WEIGHTS, ignore_index=0).to(DEVICE)\n",
    "    print(\"Loss function instantiated successfully.\")\n",
    "\n",
    "    dummy_preds = {\n",
    "        \"np\": torch.randn(BATCH_SIZE, 1, 64, 64).to(DEVICE),\n",
    "        \"hv\": torch.randn(BATCH_SIZE, 2, 64, 64).to(DEVICE),\n",
    "        \"tp\": torch.randn(BATCH_SIZE, TP_MAP_CHANNELS, 64, 64).to(DEVICE)\n",
    "    }\n",
    "    dummy_targets = {\n",
    "        \"np\": torch.randint(0, 2, (BATCH_SIZE, 1, 64, 64)).float().to(DEVICE),\n",
    "        \"hv\": torch.randn(BATCH_SIZE, 2, 64, 64).to(DEVICE),\n",
    "        \"tp\": torch.randint(0, TP_MAP_CHANNELS, (BATCH_SIZE, 64, 64)).long().to(DEVICE)\n",
    "    }\n",
    "\n",
    "    total_loss, losses = criterion(dummy_preds, dummy_targets)\n",
    "    print(f\"Loss calculation successful:\")\n",
    "    print(f\"  Total: {total_loss.item():.4f}\")\n",
    "    print(f\"  NP Loss: {losses['np'].item():.4f}\")\n",
    "    print(f\"  HV Loss: {losses['hv'].item():.4f}\")\n",
    "    print(f\"  TP Loss: {losses['tp'].item():.4f}\")\n",
    "    del criterion, dummy_preds, dummy_targets, total_loss, losses\n",
    "except Exception as e:\n",
    "    print(f\"Error during loss function test: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCDSNZPiKj4Y"
   },
   "source": [
    "## 7. Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing datasets and dataloaders...\n",
      "Initializing dataset with 167 images...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized. Found 167 valid image-mask pairs.\n",
      "Initializing dataset with 42 images...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized. Found 42 valid image-mask pairs.\n",
      "Dataloaders created.\n",
      "Loading ImageNet-pre-trained ConvNeXt-Tiny backbone from timm...\n",
      "ConvNeXt-Tiny backbone loaded successfully.\n",
      "Starting training...\n",
      "--- Epoch 1/50 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m train_loss, train_losses_breakdown = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m val_loss, val_losses_breakdown = validate_epoch(model, val_loader, criterion, DEVICE)\n\u001b[32m    156\u001b[39m end_time = time.time()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0.0\u001b[39m, epoch_losses\n\u001b[32m     21\u001b[39m pbar = tqdm(loader, desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m, total=num_batches)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# This handles batches where all items failed to load\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1434\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1436\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Runs a single training epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_losses = collections.defaultdict(float)\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    if num_batches == 0:\n",
    "        print(\"Warning: train_loader is empty.\")\n",
    "        return 0.0, epoch_losses\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False, total=num_batches)\n",
    "    for images, targets in pbar:\n",
    "        if images is None or targets is None:\n",
    "            continue\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = {k: v.to(device) for k, v in targets.items()}\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(images)\n",
    "\n",
    "        # Calculate loss\n",
    "        total_loss, losses = criterion(preds, targets)\n",
    "\n",
    "        if torch.isnan(total_loss):\n",
    "            print(\"Warning: NaN loss detected. Skipping batch.\")\n",
    "            continue\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += total_loss.item()\n",
    "        for k, v in losses.items():\n",
    "            epoch_losses[k] += v.item()\n",
    "\n",
    "        pbar.set_postfix(loss=f\"{total_loss.item():.4f}\",\n",
    "                         np=f\"{losses['np'].item():.4f}\",\n",
    "                         hv=f\"{losses['hv'].item():.4f}\",\n",
    "                         tp=f\"{losses['tp'].item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}\n",
    "    return avg_loss, avg_losses\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Runs a single validation epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_losses = collections.defaultdict(float)\n",
    "\n",
    "    num_batches = len(loader)\n",
    "    if num_batches == 0:\n",
    "        print(\"Warning: val_loader is empty.\")\n",
    "        return 0.0, epoch_losses\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc=\"Validating\", leave=False, total=num_batches)\n",
    "        for images, targets in pbar:\n",
    "            if images is None or targets is None:\n",
    "                continue\n",
    "\n",
    "            images = images.to(device)\n",
    "            targets = {k: v.to(device) for k, v in targets.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            total_loss, losses = criterion(preds, targets)\n",
    "\n",
    "            if torch.isnan(total_loss):\n",
    "                continue\n",
    "\n",
    "            epoch_loss += total_loss.item()\n",
    "            for k, v in losses.items():\n",
    "                epoch_losses[k] += v.item()\n",
    "\n",
    "            pbar.set_postfix(loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    avg_losses = {k: v / num_batches for k, v in epoch_losses.items()}\n",
    "    return avg_loss, avg_losses\n",
    "\n",
    "\n",
    "if train_ids is None or not train_ids:\n",
    "    print(\"Cannot start training: train_ids is empty. Check Stratified Split step.\")\n",
    "else:\n",
    "    print(\"Initializing datasets and dataloaders...\")\n",
    "\n",
    "    train_dataset = CellDataset(DATA_DIR, MASK_DIR, train_ids, patch_size=PATCH_SIZE, augment=True)\n",
    "    val_dataset = CellDataset(DATA_DIR, MASK_DIR, val_ids, patch_size=PATCH_SIZE, augment=False)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collate_fn=collate_fn_skip_errors, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        collate_fn=collate_fn_skip_errors,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(\"Dataloaders created.\")\n",
    "\n",
    "    model = HoverNet(num_classes=TP_MAP_CHANNELS).to(DEVICE)\n",
    "    criterion = CombinedHoverLoss(tp_class_weights=CLASS_LOSS_WEIGHTS, ignore_index=0).to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4) \n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"--- Epoch {epoch}/{EPOCHS} ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_losses_breakdown = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "        val_loss, val_losses_breakdown = validate_epoch(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_mins = (end_time - start_time) / 60\n",
    "\n",
    "        print(f\"Epoch {epoch} Summary | Time: {epoch_mins:.2f}m\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"    (NP: {train_losses_breakdown.get('np', 0):.4f}, HV: {train_losses_breakdown.get('hv', 0):.4f}, TP: {train_losses_breakdown.get('tp', 0):.4f}) \")\n",
    "        print(f\"  Valid Loss: {val_loss:.4f}\")\n",
    "        print(f\"    (NP: {val_losses_breakdown.get('np', 0):.4f}, HV: {val_losses_breakdown.get('hv', 0):.4f}, TP: {val_losses_breakdown.get('tp', 0):.4f}) \")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            print(f\"==> Validation loss improved from {best_val_loss:.4f} to {val_loss:.4f}. Saving model to {MODEL_SAVE_PATH}\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        else:\n",
    "            print(f\"Validation loss did not improve from {best_val_loss:.4f}.\")\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Model saved to {MODEL_SAVE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOTu4oDJKj4Z"
   },
   "source": [
    "## 8. Inference and Post-processing\n",
    "\n",
    "This is the final step. We will:\n",
    "1.  Load our saved `best_model.pth`.\n",
    "2.  Find all images in the `TEST_DIR`.\n",
    "3.  For each test image:\n",
    "    a.  Perform **sliding window inference** (with overlap) to get full-sized `NP`, `HV`, and `TP` prediction maps.\n",
    "    b.  Run the **post-processing** pipeline on these maps to generate instance segmentations.\n",
    "    c.  **Encode** the instances into the 4 required RLE strings.\n",
    "4.  Save all RLE strings to `submission.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJdUbDFuKj4Z"
   },
   "source": [
    "### 8a. Post-processing Pipeline\n",
    "\n",
    "This is the most complex part of the inference. We must convert the model's three output maps into final instance masks.\n",
    "\n",
    "1.  **NP Map** (`pred_np`): Threshold this to get a binary mask of all nuclei.\n",
    "2.  **HV Map** (`pred_hv`): We don't use this directly for Watershed. Instead, we use it to find *markers* (seed points) for each nucleus. We'll use the NP map and `skimage.feature.peak_local_max` with a distance transform to get good, separated markers. A more advanced method would use the HV map to find energy basins, but this is a robust start.\n",
    "3.  **TP Map** (`pred_tp`): Take an `argmax` to get a pixel-wise class prediction `(H, W)`.\n",
    "4.  **Watershed:** Use `skimage.segmentation.watershed` with our `markers` and the *inverse distance transform* of the `NP` map as the landscape. This will \"flood-fill\" from each marker until it meets another, separating touching nuclei.\n",
    "5.  **Assign Classes:** For each final instance ID from Watershed, find all the pixels belonging to it. Look up those pixels in the `TP Map (argmax)` and assign the **majority vote** (mode) class to that instance.\n",
    "6.  **Format:** Separate the instances into 4 final masks (one per class) and encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAX_fMXKKj4Z"
   },
   "outputs": [],
   "source": [
    "def post_process(pred_np, pred_hv, pred_tp, np_thresh=0.6, marker_min_dist=9):\n",
    "    \"\"\"\n",
    "    Post-processes the raw model outputs into class-specific instance masks.\n",
    "\n",
    "    Args:\n",
    "    - pred_np: (H, W) numpy array, logits or sigmoids for nucleus probability.\n",
    "    - pred_hv: (2, H, W) numpy array, (y, x) vectors.\n",
    "    - pred_tp: (5, H, W) numpy array, logits for [BG, C1, C2, C3, C4].\n",
    "    - np_thresh: Threshold to binarize the NP map.\n",
    "    - marker_min_dist: Minimum distance between seeds for watershed.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary {class_name: instance_mask} for the 4 cell types.\n",
    "    \"\"\"\n",
    "\n",
    "    if pred_np.ndim == 3:\n",
    "        pred_np = pred_np.squeeze() \n",
    "\n",
    "    if np.max(pred_np) > 1.0 or np.min(pred_np) < 0.0:\n",
    "         pred_np = 1 / (1 + np.exp(-pred_np))\n",
    "\n",
    "    binary_mask = (pred_np > np_thresh).astype(np.uint8)\n",
    "\n",
    "    type_map = np.argmax(pred_tp, axis=0).astype(np.uint8)\n",
    "\n",
    "    distance = distance_transform_edt(binary_mask)\n",
    "\n",
    "    coords = peak_local_max(distance, min_distance=marker_min_dist, labels=binary_mask)\n",
    "    markers = np.zeros_like(binary_mask, dtype=bool)\n",
    "    markers[tuple(coords.T)] = True\n",
    "\n",
    "    markers, num_features = label(markers)\n",
    "\n",
    "    if num_features == 0:\n",
    "        return {name: np.zeros(pred_np.shape, dtype=np.uint16) for name in SUBMISSION_CLASSES}\n",
    "\n",
    "    instance_map = watershed(-distance, markers, mask=binary_mask)\n",
    "\n",
    "    final_instance_masks = {}\n",
    "    for class_name in SUBMISSION_CLASSES:\n",
    "        final_instance_masks[class_name] = np.zeros(pred_np.shape, dtype=np.uint16)\n",
    "\n",
    "    instance_ids = np.unique(instance_map)[1:] # Ignore 0 (background)\n",
    "    next_instance_id_per_class = {name: 1 for name in SUBMISSION_CLASSES}\n",
    "\n",
    "    for inst_id in instance_ids:\n",
    "        inst_pixels = (instance_map == inst_id)\n",
    "\n",
    "        inst_class_idx = mode(type_map[inst_pixels], keepdims=False)[0]\n",
    "\n",
    "        if inst_class_idx == 0 or inst_class_idx not in CLASS_MAP:\n",
    "            continue\n",
    "\n",
    "        class_name = CLASS_MAP[inst_class_idx]\n",
    "\n",
    "        new_inst_id = next_instance_id_per_class[class_name]\n",
    "\n",
    "        final_instance_masks[class_name][inst_pixels] = new_inst_id\n",
    "\n",
    "        next_instance_id_per_class[class_name] += 1\n",
    "\n",
    "    return final_instance_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADSlqq3bKj4a"
   },
   "source": [
    "### 8b. Sliding Window Inference\n",
    "\n",
    "We can't pass a 4000x4000 image to the model. We must:\n",
    "1.  Create overlapping patches from the large image.\n",
    "2.  Run the model on each patch.\n",
    "3.  Stitch the predictions back together, averaging the results in the overlapping regions to get a smooth final map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syy8jmqaKj4a"
   },
   "outputs": [],
   "source": [
    "def sliding_window_inference(model, image, patch_size, overlap, device):\n",
    "    \"\"\"\n",
    "    Performs sliding window inference on a large image.\n",
    "    Returns full-sized prediction maps.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    H, W, C = image.shape\n",
    "    stride = patch_size - overlap\n",
    "\n",
    "    pad_h = (stride - (H - patch_size) % stride) % stride\n",
    "    pad_w = (stride - (W - patch_size) % stride) % stride\n",
    "\n",
    "    padded_image = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "    H_pad, W_pad, _ = padded_image.shape\n",
    "\n",
    "    pred_map_np = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "    pred_map_hv = np.zeros((2, H_pad, W_pad), dtype=np.float32)\n",
    "    pred_map_tp = np.zeros((TP_MAP_CHANNELS, H_pad, W_pad), dtype=np.float32)\n",
    "\n",
    "    count_map = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y in range(0, H_pad - patch_size + 1, stride):\n",
    "            for x in range(0, W_pad - patch_size + 1, stride):\n",
    "                patch = padded_image[y:y+patch_size, x:x+patch_size]\n",
    "\n",
    "                patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1)).float() / 255.0\n",
    "                patch_tensor = patch_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "                preds = model(patch_tensor)\n",
    "\n",
    "                pred_np_patch = preds['np'].squeeze().cpu().numpy() # (H, W)\n",
    "                pred_hv_patch = preds['hv'].squeeze().cpu().numpy() # (2, H, W)\n",
    "                pred_tp_patch = preds['tp'].squeeze().cpu().numpy() # (5, H, W)\n",
    "\n",
    "                pred_map_np[y:y+patch_size, x:x+patch_size] += pred_np_patch\n",
    "                pred_map_hv[:, y:y+patch_size, x:x+patch_size] += pred_hv_patch\n",
    "                pred_map_tp[:, y:y+patch_size, x:x+patch_size] += pred_tp_patch\n",
    "\n",
    "                count_map[y:y+patch_size, x:x+patch_size] += 1.0\n",
    "\n",
    "    final_pred_np = pred_map_np / count_map\n",
    "    final_pred_hv = pred_map_hv / count_map\n",
    "    final_pred_tp = pred_map_tp / count_map\n",
    "\n",
    "    final_pred_np = final_pred_np[0:H, 0:W]\n",
    "    final_pred_hv = final_pred_hv[:, 0:H, 0:W]\n",
    "    final_pred_tp = final_pred_tp[:, 0:H, 0:W]\n",
    "\n",
    "    return final_pred_np, final_pred_hv, final_pred_tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8k0gTfUuKSN"
   },
   "source": [
    "WPQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZbEqZgggHMw"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def sliding_window_inference(model, image, patch_size, overlap, device):\n",
    "    model.eval()\n",
    "    H, W, C = image.shape\n",
    "    stride = patch_size - overlap\n",
    "    pad_h = (stride - (H - patch_size) % stride) % stride\n",
    "    pad_w = (stride - (W - patch_size) % stride) % stride\n",
    "    padded_image = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "    H_pad, W_pad, _ = padded_image.shape\n",
    "    pred_map_np = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "    pred_map_hv = np.zeros((2, H_pad, W_pad), dtype=np.float32)\n",
    "    pred_map_tp = np.zeros((TP_MAP_CHANNELS, H_pad, W_pad), dtype=np.float32)\n",
    "    count_map = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for y in range(0, H_pad - patch_size + 1, stride):\n",
    "            for x in range(0, W_pad - patch_size + 1, stride):\n",
    "                patch = padded_image[y:y+patch_size, x:x+patch_size]\n",
    "                patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1)).float() / 255.0\n",
    "                patch_tensor = patch_tensor.unsqueeze(0).to(device)\n",
    "                preds = model(patch_tensor)\n",
    "                pred_np_patch = torch.sigmoid(preds['np']).squeeze().cpu().numpy()\n",
    "                pred_hv_patch = preds['hv'].squeeze().cpu().numpy()\n",
    "                pred_tp_patch = preds['tp'].squeeze().cpu().numpy()\n",
    "                pred_map_np[y:y+patch_size, x:x+patch_size] += pred_np_patch\n",
    "                pred_map_hv[:, y:y+patch_size, x:x+patch_size] += pred_hv_patch\n",
    "                pred_map_tp[:, y:y+patch_size, x:x+patch_size] += pred_tp_patch\n",
    "                count_map[y:y+patch_size, x:x+patch_size] += 1.0\n",
    "    count_map[count_map == 0] = 1\n",
    "    final_pred_np = pred_map_np / count_map\n",
    "    final_pred_hv = pred_map_hv / count_map\n",
    "    final_pred_tp = pred_map_tp / count_map\n",
    "    final_pred_np = final_pred_np[0:H, 0:W]\n",
    "    final_pred_hv = final_pred_hv[:, 0:H, 0:W]\n",
    "    final_pred_tp = final_pred_tp[:, 0:H, 0:W]\n",
    "    return final_pred_np, final_pred_hv, final_pred_tp\n",
    "\n",
    "def get_pq(pred_mask, gt_mask, iou_thresh=0.5):\n",
    "    pred_labels = np.unique(pred_mask)[1:]\n",
    "    gt_labels = np.unique(gt_mask)[1:]\n",
    "    if len(pred_labels) == 0 and len(gt_labels) == 0:\n",
    "        return 1.0, 0, 0, 0, 0\n",
    "    elif len(pred_labels) == 0 or len(gt_labels) == 0:\n",
    "        return 0.0, 0, 0, len(pred_labels), len(gt_labels)\n",
    "    iou_matrix = np.zeros((len(pred_labels), len(gt_labels)), dtype=np.float32)\n",
    "    for i, pred_id in enumerate(pred_labels):\n",
    "        pred_instance = (pred_mask == pred_id)\n",
    "        for j, gt_id in enumerate(gt_labels):\n",
    "            gt_instance = (gt_mask == gt_id)\n",
    "            intersection = np.logical_and(pred_instance, gt_instance).sum()\n",
    "            union = np.logical_or(pred_instance, gt_instance).sum()\n",
    "            if union > 0:\n",
    "                iou_matrix[i, j] = intersection / union\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "    tp = 0\n",
    "    iou_sum = 0.0\n",
    "    matched_pred_indices = set()\n",
    "    matched_gt_indices = set()\n",
    "    for pred_idx, gt_idx in zip(row_ind, col_ind):\n",
    "        if iou_matrix[pred_idx, gt_idx] >= iou_thresh:\n",
    "            tp += 1\n",
    "            iou_sum += iou_matrix[pred_idx, gt_idx]\n",
    "            matched_pred_indices.add(pred_idx)\n",
    "            matched_gt_indices.add(gt_idx)\n",
    "    fp = len(pred_labels) - len(matched_pred_indices)\n",
    "    fn = len(gt_labels) - len(matched_gt_indices)\n",
    "    pq = iou_sum / (tp + 0.5 * fp + 0.5 * fn + 1e-8)\n",
    "    return pq, iou_sum, tp, fp, fn\n",
    "\n",
    "def compute_pq_for_image(pred_masks_by_class, gt_masks_by_class):\n",
    "    pq_scores = {}\n",
    "    total_pq = 0.0\n",
    "    total_weight = 0.0\n",
    "    for class_name, class_weight in CLASS_WEIGHTS.items():\n",
    "        pred_mask = pred_masks_by_class.get(class_name, np.zeros_like(list(gt_masks_by_class.values())[0]))\n",
    "        gt_mask = gt_masks_by_class.get(class_name, np.zeros_like(list(gt_masks_by_class.values())[0]))\n",
    "\n",
    "        pq, iou_sum, tp, fp, fn = get_pq(pred_mask, gt_mask)\n",
    "        pq_scores[class_name] = pq\n",
    "        total_pq += pq * class_weight\n",
    "        total_weight += class_weight\n",
    "\n",
    "    wpq = total_pq / total_weight\n",
    "    return wpq, pq_scores\n",
    "\n",
    "def evaluate_model_wpq(model, image_ids, device):\n",
    "    model.eval()\n",
    "    all_wpq_scores = []\n",
    "    all_pq_by_class = collections.defaultdict(list)\n",
    "\n",
    "    print(f\"Starting wPQ evaluation on {len(image_ids)} validation images...\")\n",
    "\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "\n",
    "        print(f\"\\n[Image {i+1}/{len(image_ids)}] STARTING: {image_id}\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            img_path = os.path.join(DATA_DIR, f\"{image_id}.tif\")\n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"  Error: Image file not found at {img_path}. Skipping.\")\n",
    "                continue\n",
    "            image = tifffile.imread(img_path)\n",
    "\n",
    "            if image.ndim == 3 and image.shape[-1] == 4: image = image[:, :, :3]\n",
    "            if image.ndim == 2: image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            if image.dtype != np.uint8:\n",
    "                if image.max() > 255: image = (image / image.max() * 255).astype(np.uint8)\n",
    "                else: image = image.astype(np.uint8)\n",
    "            H, W, _ = image.shape\n",
    "            print(f\"  Loaded image {image_id} with shape ({H}, {W})\")\n",
    "\n",
    "            print(f\"  Running sliding window inference...\")\n",
    "            pred_np, pred_hv, pred_tp = sliding_window_inference(\n",
    "                model, image, PATCH_SIZE, OVERLAP, device\n",
    "            )\n",
    "\n",
    "            print(f\"  Running post-processing...\")\n",
    "            pred_masks_by_class = post_process(pred_np, pred_hv, pred_tp, np_thresh=0.6)\n",
    "\n",
    "            print(f\"  Loading ground truth masks...\")\n",
    "            mask_path = os.path.join(MASK_DIR, f\"{image_id}.npz\")\n",
    "            if not os.path.exists(mask_path):\n",
    "                print(f\"  Error: Mask file not found at {mask_path}. Skipping.\")\n",
    "                continue\n",
    "            with np.load(mask_path) as data:\n",
    "                gt_instance_map = data['instance_map']\n",
    "                gt_type_map = data['type_map']\n",
    "\n",
    "            gt_masks_by_class = {}\n",
    "            for class_id, class_name in CLASS_MAP.items():\n",
    "                class_instances = np.unique(gt_instance_map[gt_type_map == class_id])\n",
    "                class_instances = class_instances[class_instances != 0]\n",
    "                gt_mask = np.zeros((H, W), dtype=np.uint16)\n",
    "                for new_id, inst_id in enumerate(class_instances, 1):\n",
    "                    gt_mask[gt_instance_map == inst_id] = new_id\n",
    "                gt_masks_by_class[class_name] = gt_mask\n",
    "\n",
    "            print(f\"  Calculating wPQ score...\")\n",
    "            wpq, pq_by_class = compute_pq_for_image(pred_masks_by_class, gt_masks_by_class)\n",
    "\n",
    "            all_wpq_scores.append(wpq)\n",
    "            for class_name, pq in pq_by_class.items():\n",
    "                all_pq_by_class[class_name].append(pq)\n",
    "\n",
    "            end_time = time.time()\n",
    "            print(f\"  FINISHED: {image_id} | Time: {end_time - start_time:.2f}s | wPQ: {wpq:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  --- CRITICAL ERROR on {image_id}: {e}. Skipping. ---\")\n",
    "            continue\n",
    "\n",
    "    avg_wpq = np.mean(all_wpq_scores) if all_wpq_scores else 0.0\n",
    "    avg_pq_by_class = pd.Series(\n",
    "        {class_name: np.mean(scores) for class_name, scores in all_pq_by_class.items()}\n",
    "    )\n",
    "\n",
    "    return avg_wpq, avg_pq_by_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4cdXPCu1b-9",
    "outputId": "bd93378a-6a3c-4c73-fc85-f1e0c6595bf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from best_model_new_2.pth...\n",
      "Finding 'fast' validation images...\n",
      "Total validation images: 42\n",
      "Found 38 images below 1000000 pixels (fast set).\n",
      "Found 4 images above threshold (slow set): ['slide93', 'slide61', 'slide162', 'slide72']\n",
      "---\n",
      "Loading ImageNet-pre-trained ConvNeXt-Tiny backbone from timm...\n",
      "ConvNeXt-Tiny backbone loaded successfully.\n",
      "Model loaded successfully.\n",
      "Starting FAST wPQ evaluation on 38 validation images...\n",
      "Starting wPQ evaluation on 38 validation images...\n",
      "\n",
      "[Image 1/38] STARTING: slide158\n",
      "  Loaded image slide158 with shape (488, 586)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide158 | Time: 0.42s | wPQ: 0.7897\n",
      "\n",
      "[Image 2/38] STARTING: slide32\n",
      "  Loaded image slide32 with shape (270, 480)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide32 | Time: 0.35s | wPQ: 0.6772\n",
      "\n",
      "[Image 3/38] STARTING: slide84\n",
      "  Loaded image slide84 with shape (246, 226)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide84 | Time: 0.06s | wPQ: 0.7908\n",
      "\n",
      "[Image 4/38] STARTING: slide24\n",
      "  Loaded image slide24 with shape (638, 686)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide24 | Time: 39.74s | wPQ: 0.7220\n",
      "\n",
      "[Image 5/38] STARTING: slide77\n",
      "  Loaded image slide77 with shape (850, 910)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide77 | Time: 106.67s | wPQ: 0.9336\n",
      "\n",
      "[Image 6/38] STARTING: slide110\n",
      "  Loaded image slide110 with shape (709, 1074)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide110 | Time: 61.24s | wPQ: 0.4871\n",
      "\n",
      "[Image 7/38] STARTING: slide63\n",
      "  Loaded image slide63 with shape (121, 128)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide63 | Time: 0.07s | wPQ: 0.9267\n",
      "\n",
      "[Image 8/38] STARTING: slide78\n",
      "  Loaded image slide78 with shape (199, 192)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide78 | Time: 0.08s | wPQ: 0.7436\n",
      "\n",
      "[Image 9/38] STARTING: slide194\n",
      "  Loaded image slide194 with shape (514, 488)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide194 | Time: 26.93s | wPQ: 0.9822\n",
      "\n",
      "[Image 10/38] STARTING: slide45\n",
      "  Loaded image slide45 with shape (524, 598)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide45 | Time: 5.32s | wPQ: 0.4865\n",
      "\n",
      "[Image 11/38] STARTING: slide62\n",
      "  Loaded image slide62 with shape (90, 98)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide62 | Time: 0.05s | wPQ: 0.9295\n",
      "\n",
      "[Image 12/38] STARTING: slide25\n",
      "  Loaded image slide25 with shape (166, 261)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide25 | Time: 0.09s | wPQ: 0.2791\n",
      "\n",
      "[Image 13/38] STARTING: slide20\n",
      "  Loaded image slide20 with shape (311, 286)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide20 | Time: 0.16s | wPQ: 0.5239\n",
      "\n",
      "[Image 14/38] STARTING: slide130\n",
      "  Loaded image slide130 with shape (591, 778)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide130 | Time: 0.68s | wPQ: 0.5434\n",
      "\n",
      "[Image 15/38] STARTING: slide167\n",
      "  Loaded image slide167 with shape (149, 288)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide167 | Time: 0.10s | wPQ: 0.7317\n",
      "\n",
      "[Image 16/38] STARTING: slide176\n",
      "  Loaded image slide176 with shape (415, 477)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide176 | Time: 4.78s | wPQ: 0.4856\n",
      "\n",
      "[Image 17/38] STARTING: slide156\n",
      "  Loaded image slide156 with shape (804, 744)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide156 | Time: 20.38s | wPQ: 0.4845\n",
      "\n",
      "[Image 18/38] STARTING: slide118\n",
      "  Loaded image slide118 with shape (214, 187)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide118 | Time: 0.05s | wPQ: 0.7195\n",
      "\n",
      "[Image 19/38] STARTING: slide108\n",
      "  Loaded image slide108 with shape (468, 532)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide108 | Time: 35.53s | wPQ: 0.4843\n",
      "\n",
      "[Image 20/38] STARTING: slide168\n",
      "  Loaded image slide168 with shape (877, 802)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide168 | Time: 18.96s | wPQ: 0.0291\n",
      "\n",
      "[Image 21/38] STARTING: slide68\n",
      "  Loaded image slide68 with shape (224, 284)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide68 | Time: 0.11s | wPQ: 0.7643\n",
      "\n",
      "[Image 22/38] STARTING: slide141\n",
      "  Loaded image slide141 with shape (512, 512)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide141 | Time: 1.18s | wPQ: 0.1892\n",
      "\n",
      "[Image 23/38] STARTING: slide1\n",
      "  Loaded image slide1 with shape (297, 204)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide1 | Time: 0.09s | wPQ: 0.6752\n",
      "\n",
      "[Image 24/38] STARTING: slide107\n",
      "  Loaded image slide107 with shape (127, 305)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide107 | Time: 0.08s | wPQ: 0.8086\n",
      "\n",
      "[Image 25/38] STARTING: slide106\n",
      "  Loaded image slide106 with shape (352, 491)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide106 | Time: 0.45s | wPQ: 0.8074\n",
      "\n",
      "[Image 26/38] STARTING: slide144\n",
      "  Loaded image slide144 with shape (205, 382)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide144 | Time: 0.12s | wPQ: 0.4011\n",
      "\n",
      "[Image 27/38] STARTING: slide11\n",
      "  Loaded image slide11 with shape (694, 824)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide11 | Time: 0.89s | wPQ: 0.7572\n",
      "\n",
      "[Image 28/38] STARTING: slide136\n",
      "  Loaded image slide136 with shape (173, 74)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide136 | Time: 0.04s | wPQ: 0.8015\n",
      "\n",
      "[Image 29/38] STARTING: slide189\n",
      "  Loaded image slide189 with shape (287, 560)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide189 | Time: 0.22s | wPQ: 0.5562\n",
      "\n",
      "[Image 30/38] STARTING: slide60\n",
      "  Loaded image slide60 with shape (763, 756)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide60 | Time: 34.58s | wPQ: 0.4719\n",
      "\n",
      "[Image 31/38] STARTING: slide31\n",
      "  Loaded image slide31 with shape (712, 414)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide31 | Time: 1.23s | wPQ: 0.1997\n",
      "\n",
      "[Image 32/38] STARTING: slide44\n",
      "  Loaded image slide44 with shape (624, 734)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide44 | Time: 51.65s | wPQ: 0.4827\n",
      "\n",
      "[Image 33/38] STARTING: slide186\n",
      "  Loaded image slide186 with shape (266, 185)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide186 | Time: 0.08s | wPQ: 0.4457\n",
      "\n",
      "[Image 34/38] STARTING: slide64\n",
      "  Loaded image slide64 with shape (402, 618)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide64 | Time: 3.05s | wPQ: 0.0685\n",
      "\n",
      "[Image 35/38] STARTING: slide140\n",
      "  Loaded image slide140 with shape (512, 512)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide140 | Time: 11.17s | wPQ: 0.5292\n",
      "\n",
      "[Image 36/38] STARTING: slide102\n",
      "  Loaded image slide102 with shape (251, 483)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide102 | Time: 0.33s | wPQ: 0.7338\n",
      "\n",
      "[Image 37/38] STARTING: slide96\n",
      "  Loaded image slide96 with shape (512, 512)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide96 | Time: 1.60s | wPQ: 0.0295\n",
      "\n",
      "[Image 38/38] STARTING: slide150\n",
      "  Loaded image slide150 with shape (788, 636)\n",
      "  Running sliding window inference...\n",
      "  Running post-processing...\n",
      "  Loading ground truth masks...\n",
      "  Calculating wPQ score...\n",
      "  FINISHED: slide150 | Time: 49.56s | wPQ: 0.4834\n",
      "\n",
      "--- FAST Evaluation Complete ---\n",
      "  (Score is based on 38 / 42 images)\n",
      "  Average wPQ Score: 0.5778\n",
      "\n",
      "  Average PQ by Class:\n",
      "Epithelial   0.5808\n",
      "Lymphocyte   0.4861\n",
      "Neutrophil   0.4662\n",
      "Macrophage   0.6983\n"
     ]
    }
   ],
   "source": [
    "CLASS_MAP = {\n",
    "    1: 'Epithelial',\n",
    "    2: 'Lymphocyte',\n",
    "    3: 'Neutrophil',\n",
    "    4: 'Macrophage'\n",
    "}\n",
    "CLASS_WEIGHTS = {\n",
    "    \"Epithelial\": 1,\n",
    "    \"Lymphocyte\": 1,\n",
    "    \"Neutrophil\": 10,\n",
    "    \"Macrophage\": 10\n",
    "}\n",
    "DATA_DIR = \"train\"\n",
    "MASK_DIR = \"mask_new\"\n",
    "TEST_DIR = \"test_final\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PATCH_SIZE = 256\n",
    "OVERLAP = 64\n",
    "TP_MAP_CHANNELS = len(CLASS_MAP) + 1 # BG + 4 classes\n",
    "MODEL_SAVE_PATH = \"best_model_new.pth\"\n",
    "\n",
    "print(f\"Loading best model from {MODEL_SAVE_PATH}...\")\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"Error: Model file not found at {MODEL_SAVE_PATH}. Please run training first.\")\n",
    "elif 'val_ids' not in locals() or not val_ids:\n",
    "    print(\"Error: 'val_ids' not found. Please run the stratified split cell first.\")\n",
    "else:\n",
    "    print(\"Finding 'fast' validation images...\")\n",
    "\n",
    "    PIXEL_THRESHOLD = 1000000\n",
    "\n",
    "    val_ids_fast = []\n",
    "    val_ids_slow = []\n",
    "\n",
    "    for image_id in val_ids:\n",
    "        img_path = os.path.join(DATA_DIR, f\"{image_id}.tif\")\n",
    "        try:\n",
    "            with tifffile.TiffFile(img_path) as tif:\n",
    "                shape = tif.pages[0].shape\n",
    "\n",
    "            num_pixels = shape[0] * shape[1]\n",
    "\n",
    "            if num_pixels < PIXEL_THRESHOLD:\n",
    "                val_ids_fast.append(image_id)\n",
    "            else:\n",
    "                val_ids_slow.append(image_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not read shape for {image_id}. Error: {e}. Adding to fast list.\")\n",
    "            val_ids_fast.append(image_id)\n",
    "\n",
    "    print(f\"Total validation images: {len(val_ids)}\")\n",
    "    print(f\"Found {len(val_ids_fast)} images below {PIXEL_THRESHOLD} pixels (fast set).\")\n",
    "    print(f\"Found {len(val_ids_slow)} images above threshold (slow set): {val_ids_slow}\")\n",
    "    print(\"---\")\n",
    "\n",
    "    model_for_eval = HoverNet(num_classes=TP_MAP_CHANNELS).to(DEVICE)\n",
    "    model_for_eval.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    model_for_eval.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    print(f\"Starting FAST wPQ evaluation on {len(val_ids_fast)} validation images...\")\n",
    "    avg_wpq, avg_pq_by_class = evaluate_model_wpq(model_for_eval, val_ids_fast, DEVICE)\n",
    "\n",
    "    print(\"\\n--- FAST Evaluation Complete ---\")\n",
    "    print(f\"  (Score is based on {len(val_ids_fast)} / {len(val_ids)} images)\")\n",
    "    print(f\"  Average wPQ Score: {avg_wpq:.4f}\")\n",
    "    print(\"\\n  Average PQ by Class:\")\n",
    "    print(avg_pq_by_class.to_string(float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6NIeo3nKj4a"
   },
   "source": [
    "### 8c. Generate Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 674,
     "referenced_widgets": [
      "1fc0079ccaa542a799c6466728311bcd",
      "cefe90ff1d064048bb1d0dadc087ce63",
      "d3fcc57fc0054154a462d0fed384b003",
      "1d6e6e8781eb4f58851f42eb82fa2498",
      "0b119502bccf4b70afbb0930397f58c5",
      "25a7edba1ff14504b8daa7b250abfb33",
      "664e3c31786e4898b375e7554d230e35",
      "3cd54274d10e4f77bd8f7cbd036088f7",
      "c542df8955b44b348b4ba2bc20133571",
      "96cf27130f6942da9b956432f1c65b7f",
      "9481f3f1be364cce8da9abec85df443c"
     ]
    },
    "id": "qjKVX1bFKj4a",
    "outputId": "7ee8ee15-bb9f-4d72-9343-b8d1afdb2bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Inference and Submission Generation ---\n",
      "Loading model from best_model_new_2.pth...\n",
      "Loading ImageNet-pre-trained ConvNeXt-Tiny backbone from timm...\n",
      "ConvNeXt-Tiny backbone loaded successfully.\n",
      "Model loaded successfully.\n",
      "Found 40 test images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file saved to: submission_new_2.csv\n",
      "  image_id                                         Epithelial  \\\n",
      "0   slide1                                                  0   \n",
      "1  slide10                                                  0   \n",
      "2  slide11  64 308 4 114 572 22 64 938 11 114 1204 25 64 1...   \n",
      "3  slide12                                                  0   \n",
      "4  slide13                                                  0   \n",
      "\n",
      "                                          Lymphocyte  \\\n",
      "0  1 31121 4 1 31276 8 1 31432 10 1 31588 12 1 31...   \n",
      "1  527 500 17 527 1135 18 527 1769 19 440 2327 6 ...   \n",
      "2  1 414286 7 1 414917 11 1 415549 13 1 416181 15...   \n",
      "3                                                  0   \n",
      "4                                                  0   \n",
      "\n",
      "                                          Neutrophil  \\\n",
      "0                                                  0   \n",
      "1  1 507800 7 1 508433 11 1 509067 14 1 509700 17...   \n",
      "2                                                  0   \n",
      "3  8 3669 6 8 3873 10 8 4077 13 8 4282 15 8 4487 ...   \n",
      "4  2 2216 6 2 2386 8 2 2556 9 2 2726 10 2 2897 9 ...   \n",
      "\n",
      "                                          Macrophage  \n",
      "0  5 1855 2 5 2007 11 5 2161 17 5 2316 21 5 2472 ...  \n",
      "1                                                  0  \n",
      "2                                                  0  \n",
      "3                                                  0  \n",
      "4  12 107 23 12 276 28 12 446 29 12 617 31 12 787...  \n"
     ]
    }
   ],
   "source": [
    "def generate_submission(model_path, test_dir, submission_path):\n",
    "    print(\"--- Starting Inference and Submission Generation ---\")\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at {model_path}. Cannot generate submission.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = HoverNet(num_classes=TP_MAP_CHANNELS).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    test_image_paths = sorted(glob.glob(os.path.join(test_dir, \"*.tif\")))\n",
    "    if not test_image_paths:\n",
    "        print(f\"Error: No .tif images found in {test_dir}. Cannot generate submission.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(test_image_paths)} test images.\")\n",
    "\n",
    "    submission_data = []\n",
    "\n",
    "    for img_path in tqdm(test_image_paths, desc=\"Processing Test Images\"):\n",
    "        image_id = os.path.basename(img_path).split('.')[0]\n",
    "\n",
    "        try:\n",
    "            image = tifffile.imread(img_path)\n",
    "\n",
    "            if image.ndim == 3 and image.shape[-1] == 4:\n",
    "                image = image[:, :, :3]\n",
    "            if image.ndim == 2:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "            if image.dtype != np.uint8:\n",
    "                if image.max() > 255:\n",
    "                    image = (image / image.max() * 255).astype(np.uint8)\n",
    "                else:\n",
    "                    image = image.astype(np.uint8)\n",
    "\n",
    "            pred_np, pred_hv, pred_tp = sliding_window_inference(\n",
    "                model, image,\n",
    "                patch_size=PATCH_SIZE,\n",
    "                overlap=OVERLAP,\n",
    "                device=DEVICE\n",
    "            )\n",
    "\n",
    "            instance_masks_by_class = post_process(\n",
    "                pred_np, pred_hv, pred_tp,\n",
    "                np_thresh=0.5,\n",
    "                marker_min_dist=9\n",
    "            )\n",
    "\n",
    "            rle_strings = {}\n",
    "            for class_name in SUBMISSION_CLASSES:\n",
    "                mask = instance_masks_by_class[class_name]\n",
    "                rle_strings[class_name] = rle_encode_instance_mask(mask)\n",
    "\n",
    "            submission_data.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"Epithelial\": rle_strings['Epithelial'],\n",
    "                \"Lymphocyte\": rle_strings['Lymphocyte'],\n",
    "                \"Neutrophil\": rle_strings['Neutrophil'],\n",
    "                \"Macrophage\": rle_strings['Macrophage']\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_id}. Appending '0' for all classes. Error: {e}\")\n",
    "            submission_data.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"Epithelial\": \"0\",\n",
    "                \"Lymphocyte\": \"0\",\n",
    "                \"Neutrophil\": \"0\",\n",
    "                \"Macrophage\": \"0\"\n",
    "            })\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df = submission_df[[\"image_id\"] + SUBMISSION_CLASSES]\n",
    "\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"\\nSubmission file saved to: {submission_path}\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "generate_submission(\n",
    "    model_path=MODEL_SAVE_PATH,\n",
    "    test_dir=TEST_DIR,\n",
    "    submission_path=SUBMISSION_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing Grid Search to calculate threshold and window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "675b6726663c468d89c35149259f2ae7",
      "a6ee45decc5e4aa4b842a22f615446d7",
      "32acec25efd14085aebe096ee9e06b1f",
      "93cef5f001fe42759257f7777cbdb3e3",
      "9d3a330a5bee4a38b31d69bb66f3cc94",
      "93876077f12e4931958eeddceaaf9f28",
      "37518002c8ef4ae2a7d9d8eafa02ede4",
      "0914cbbca1744b8cb33970fe5efb2238",
      "669808af0ff849f29e8d4bbc610451ee",
      "5502d6a601f3498eaaecc18b991974f8",
      "115c1422f0a84c61bd2ac4ce81cf8492",
      "107ec9307f6d4c9fab929a27188bc335",
      "a3d84fac978043628ac637ba6bcc100b",
      "c07cd9de0bb74e84986547dcc9f17590",
      "ad91f9e444f44d62bcd1c0a8f4cc2583",
      "5ebd843f9f8544388482743a67628b24",
      "0a0de13f202440f8ac3f2a262264891b",
      "4092df040a1542bc903d297a8c48eb14",
      "012a6bc56d4b4e099957078c777793e1",
      "d613b73021914f6b97ab8fd60c1320d1",
      "9dfc49d86c2948b5b0ffda0d7711050e",
      "e4885d526a0d41418dc429fe84e69dea",
      "67958de8fd1e4a8da6e80df76b5ec63e",
      "09b3a807b3d24474beb93b081cba7e8d",
      "a50ff51b095e4e34817ae1baa4e338d4",
      "46c0afca88034db7aa5eaef01c1d1d12",
      "229dab9a94f34d1b90f3bd44d7dd5e71",
      "2fce4f6d18d942518bda9a0b580e7c3a",
      "4e465655faa244f093bf29673bc906db",
      "dad4f745b70d4f13a7ea51bd1aa2d451",
      "16297cded3484cfea6d9af3044da1201",
      "38b19bec87ac4ac7948afdd059cb5205",
      "0e016af174d74c1190b5b030c6aeec56",
      "139e3afbccee49888330a3b73fe1aa5f",
      "92dcd73955d34379bdd8a39bebba132a",
      "3005f6e1326d4f328e67475780a87d5f",
      "99947954c60e43cca4f536e751d74381",
      "33ea8bf89c2e48d2bf4f35b9dc6a2530",
      "4442b371102d461f80e2ca018b220797",
      "b206f8d7a117482eac761cfaafb885bc",
      "d39a0eeca7ab44bb889ceadaea3798dd",
      "001bfa93b0ed45a29a8bc8700abba499",
      "9445efe6b14448898cccf6d88961020f",
      "617d99c4606441daaca2916a9da72ad7",
      "182da6cde54749089ce1bf168161857b",
      "83e703b599e840968fea8ea842466631",
      "3aa821229f494190826591829763ced3",
      "f73499731d2a4608be03b389f1aeeb1e",
      "03d33c0cb60142f79a089d3fb57fc799",
      "3cd64fc50cf14424bf7ec076cb896d89",
      "2dba7f86c03c4dc4bc26821deca6850d",
      "25cfa5071a24488da4441a826d55f1be",
      "6d8c0a8cc8a742b5b72aa260f31abfb0",
      "05b28977b7554353b91b4ab6249163c9",
      "b905c3505ec04d1faddec23b16623416",
      "726847cd9494484f86cbc616323ebf8e",
      "9bb14a5fbcba4b7ca8a959f461c05315",
      "66adc0c96adb40198f02533e7b799bee",
      "e0b5a0c1824a454f8219136a932dbf2e",
      "4cb879de0dfc4b50984281510a9290aa",
      "17fdae60032d4b718222a06bec6810b8",
      "ef9f05178b234cf281a0977203c766a9",
      "0760300e40c040d8a36401176db54bd9",
      "dd93ffc68e40409e9ac4a2f48d8fbd10",
      "160c843762174bb1adb6325835f536be",
      "4e80a02b99b747488c19a0b99b93cd44",
      "a8851224a3d84d45a38727e7507a7e4a",
      "2bda6eee7d204aa1bb042a2dd157d35c",
      "6c183609c70d47a78f7d016fbe37f54f",
      "b22b34d672c241ba9905dc8159bfc110",
      "3da3d1fc6d664238800681b7187706c9",
      "a684761d9db74741a7a510940c51e093",
      "eca5651101754884a1b7185e9a76ac16",
      "445c4d31f3e9435db759a5e47376e361",
      "769d658b8e41488bb6cb5e7dedd38138",
      "e7a6eeede91f4fe68187943bb357eae4",
      "659038ebe8eb42ecb2d8cae12b478137",
      "9101188eca3e429b9c2b78d5ab7da9c3",
      "525edf5b42374d99bcafdc19f4152cd0",
      "611be31fae824f629fc3de3c98266e77",
      "f703c170478b4aeea7895f7a4482e68a",
      "bae54fe482e74ad6b2f43f87b0c30e88",
      "2f1930c5e5ea4a1e963fab80ad922ae8",
      "7d6d2a24662f418cb40f50e21d42c641",
      "3a65633d2ed64333aa3645c855da8edd",
      "446349cbc06740ea8ed0d76a66106201",
      "f91db146f5b548b5b8b111756ec7fe8a",
      "b42a21deb9a34d51bd4a4fd83c009563",
      "13085ebeef164ee491c6b06c2b9029d3",
      "d8346f70c5a6491dbecf04f39f584133",
      "7be75d9be441414faaaae9d2222ae1c5",
      "cab31a9b6c214efda6e2deca46ec62f3",
      "29f8d28fa91843d899a63ae79f022bdd",
      "c8900f2cf3814d28974fa4ef285a790f",
      "452584e101324b2eb2ef3df0d7e1d12f",
      "81c4ace1cf28410892afa306ad76cbb8",
      "e40dce98940043f4bfd4593c259d733f",
      "8c20237be0054ea2bef07373ea4a21e8",
      "a2b9d0e6e33241d9a7051e33ac87a4a5",
      "c83df8a99d554da88a366e70c022e04b",
      "d0ada0f18ade45dc88bc1e28ff951c85",
      "97d18cb701854350840fec3d0d4a8566",
      "64084b9c4c8d4d029e17179deee2419b",
      "c60c89741032447f9fd8237ff5592718",
      "dde9e04f61774cc2b4eeffa99b2bde48",
      "d6d5d1be384f45859d85072afd6f1df1",
      "4cfeef3d94204fe592f5316f361dbd01",
      "58ee358d367d47859f7c17a27b325292",
      "25873fb3b194463dbb5e6281e9eebfec",
      "cf389c0ce481416bbece983432e5a1aa",
      "2016a18625724f3a9e30c469ae5e6484",
      "8c5a590cd42c4079bae8a4feae395bae",
      "9d861c260be640ee81368e491f9b69da",
      "e30fc1908383480baa883f12bbf4ff60",
      "727a42a8317b411c9ede92e632c76e12",
      "8bbf6c0dbed54f8190593823db07d3fe",
      "0fa0def5156d4759867e5c7cc0ed1d70",
      "ee090908fa924fbdbf26dd05f771d86d",
      "8a7199ef5a11411b9ce4531ec402aa09",
      "3d4a6d63e0e54b9780d44cf76c15d2cf",
      "5ecbacd4401b4f6882d34c6b033146f5",
      "005bec4211a34a73bee7be364119dfb9",
      "913d9842fd2746f6aa3f5a857b2f6559",
      "1af146e32cd34168a34fa640cd699f17",
      "369f0c42dc8049c39df4314a7307e1c7",
      "d24ad04423c44fa0aaaed791eae8b190",
      "c1361f58fddc40da99b49fa43af24a62",
      "7321b1a765cf428a950bd358e13acb40",
      "d50acdf41bb7491687bfd83edce21e9e",
      "67c332c3303a4cc9b11e03a21b2b2f91",
      "e90f5bca44f64116a4cc2762df7f03be",
      "8791fa5132f84fb0b34149f6029af4c2"
     ]
    },
    "id": "ZbCMU_mZLUVt",
    "outputId": "68f7f447-c26b-49da-dced-a45d964fdbeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model from best_model_new_3.pth...\n",
      "Finding 'fast' validation images...\n",
      "Running 2D Grid Search on 31 fast images...\n",
      "Loading ImageNet-pre-trained ConvNeXt-Tiny backbone from timm...\n",
      "ConvNeXt-Tiny backbone loaded successfully.\n",
      "Model loaded successfully.\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.5, min_dist = 5) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.5, d=5): avg_wPQ = 0.6566\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.6, min_dist = 5) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.6, d=5): avg_wPQ = 0.6314\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.7, min_dist = 5) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.7, d=5): avg_wPQ = 0.6282\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.8, min_dist = 5) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.8, d=5): avg_wPQ = 0.6023\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.5, min_dist = 7) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.5, d=7): avg_wPQ = 0.6714\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.6, min_dist = 7) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.6, d=7): avg_wPQ = 0.6420\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.7, min_dist = 7) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.7, d=7): avg_wPQ = 0.6325\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.8, min_dist = 7) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.8, d=7): avg_wPQ = 0.6083\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.5, min_dist = 9) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.5, d=9): avg_wPQ = 0.6763\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.6, min_dist = 9) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.6, d=9): avg_wPQ = 0.6456\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.7, min_dist = 9) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Result for (t=0.7, d=9): avg_wPQ = 0.6381\n",
      "\n",
      "--- Testing combo: (np_thresh = 0.8, min_dist = 9) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 319\u001b[39m\n\u001b[32m    316\u001b[39m     gt_masks_by_class[class_name] = gt_mask\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# --- 5. Compute Score ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m wpq, _ = \u001b[43mcompute_pq_for_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_masks_by_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_masks_by_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m all_wpq_scores.append(wpq)\n\u001b[32m    321\u001b[39m pbar.set_postfix(avg_wPQ=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(all_wpq_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 217\u001b[39m, in \u001b[36mcompute_pq_for_image\u001b[39m\u001b[34m(pred_masks_by_class, gt_masks_by_class)\u001b[39m\n\u001b[32m    214\u001b[39m pred_mask = pred_masks_by_class.get(class_name, np.zeros(ref_shape, dtype=np.uint16))\n\u001b[32m    215\u001b[39m gt_mask = gt_masks_by_class.get(class_name, np.zeros(ref_shape, dtype=np.uint16))\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m pq, iou_sum, tp, fp, fn = \u001b[43mget_pq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m pq_scores[class_name] = pq\n\u001b[32m    219\u001b[39m total_pq += pq * class_weight\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mget_pq\u001b[39m\u001b[34m(pred_mask, gt_mask, iou_thresh)\u001b[39m\n\u001b[32m    167\u001b[39m gt_instance = (gt_mask == gt_id)\n\u001b[32m    169\u001b[39m intersection = np.logical_and(pred_instance, gt_instance).sum()\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m union = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlogical_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgt_instance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m union > \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     iou_matrix[i, j] = intersection / union\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniforge3/lib/python3.12/site-packages/numpy/_core/_methods.py:52\u001b[39m, in \u001b[36m_sum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     51\u001b[39m          initial=_NoValue, where=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "CLASS_MAP = {\n",
    "    1: 'Epithelial',\n",
    "    2: 'Lymphocyte',\n",
    "    3: 'Neutrophil',\n",
    "    4: 'Macrophage'\n",
    "}\n",
    "CLASS_WEIGHTS = {\n",
    "    \"Epithelial\": 1,\n",
    "    \"Lymphocyte\": 1,\n",
    "    \"Neutrophil\": 10,\n",
    "    \"Macrophage\": 10\n",
    "}\n",
    "DATA_DIR = \"train\"\n",
    "MASK_DIR = \"mask_new\"\n",
    "TEST_DIR = \"test_final\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PATCH_SIZE = 256\n",
    "OVERLAP = 64\n",
    "TP_MAP_CHANNELS = len(CLASS_MAP) + 1 # BG + 4 classes\n",
    "MODEL_SAVE_PATH = \"best_model_new_3.pth\"\n",
    "\n",
    "def post_process(pred_np, pred_hv, pred_tp, np_thresh=0.5, marker_min_dist=7):\n",
    "    \"\"\"\n",
    "    Post-processes the raw model outputs into class-specific instance masks.\n",
    "    pred_np is expected to be probabilities (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    binary_mask = (pred_np > np_thresh).astype(np.uint8)\n",
    "\n",
    "    type_map = np.argmax(pred_tp, axis=0).astype(np.uint8)\n",
    "\n",
    "    distance = distance_transform_edt(binary_mask)\n",
    "    coords = peak_local_max(distance, min_distance=marker_min_dist, labels=binary_mask)\n",
    "    markers = np.zeros_like(binary_mask, dtype=bool)\n",
    "    markers[tuple(coords.T)] = True\n",
    "    markers, num_features = label(markers)\n",
    "\n",
    "    if num_features == 0:\n",
    "        return {name: np.zeros(pred_np.shape, dtype=np.uint16) for name in CLASS_MAP.values()}\n",
    "\n",
    "    instance_map = watershed(-distance, markers, mask=binary_mask)\n",
    "\n",
    "    final_instance_masks = {}\n",
    "    for class_name in CLASS_MAP.values():\n",
    "        final_instance_masks[class_name] = np.zeros(pred_np.shape, dtype=np.uint16)\n",
    "\n",
    "    next_instance_id_per_class = {name: 1 for name in CLASS_MAP.values()}\n",
    "\n",
    "    instance_ids = np.unique(instance_map)[1:] # Ignore 0\n",
    "    for inst_id in instance_ids:\n",
    "        inst_pixels = (instance_map == inst_id)\n",
    "\n",
    "        inst_class_idx_mode = mode(type_map[inst_pixels], keepdims=False)\n",
    "\n",
    "        if inst_class_idx_mode.count.size == 0:\n",
    "            continue # Skip if instance has no typed pixels (shouldn't happen)\n",
    "\n",
    "        inst_class_idx = inst_class_idx_mode.mode[0] if isinstance(inst_class_idx_mode.mode, np.ndarray) else inst_class_idx_mode.mode\n",
    "\n",
    "        if inst_class_idx == 0 or inst_class_idx not in CLASS_MAP:\n",
    "            continue\n",
    "\n",
    "        class_name = CLASS_MAP[inst_class_idx]\n",
    "        new_inst_id = next_instance_id_per_class[class_name]\n",
    "        final_instance_masks[class_name][inst_pixels] = new_inst_id\n",
    "        next_instance_id_per_class[class_name] += 1\n",
    "\n",
    "    return final_instance_masks\n",
    "\n",
    "def sliding_window_inference(model, image, patch_size, overlap, device):\n",
    "    \"\"\"\n",
    "    Performs sliding window inference on a large image.\n",
    "    Returns full-sized prediction maps.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    H, W, C = image.shape\n",
    "    stride = patch_size - overlap\n",
    "\n",
    "    pad_h = (stride - (H - patch_size) % stride) % stride\n",
    "    pad_w = (stride - (W - patch_size) % stride) % stride\n",
    "\n",
    "    padded_image = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode='constant')\n",
    "    H_pad, W_pad, _ = padded_image.shape\n",
    "\n",
    "    pred_map_np = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "    pred_map_hv = np.zeros((2, H_pad, W_pad), dtype=np.float32)\n",
    "    pred_map_tp = np.zeros((TP_MAP_CHANNELS, H_pad, W_pad), dtype=np.float32)\n",
    "    count_map = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for y in range(0, H_pad - patch_size + 1, stride):\n",
    "            for x in range(0, W_pad - patch_size + 1, stride):\n",
    "                patch = padded_image[y:y+patch_size, x:x+patch_size]\n",
    "                patch_tensor = torch.from_numpy(patch.transpose(2, 0, 1)).float() / 255.0\n",
    "                patch_tensor = patch_tensor.unsqueeze(0).to(device)\n",
    "                preds = model(patch_tensor)\n",
    "\n",
    "                pred_np_patch = torch.sigmoid(preds['np']).squeeze().cpu().numpy()\n",
    "                pred_hv_patch = preds['hv'].squeeze().cpu().numpy()\n",
    "                pred_tp_patch = preds['tp'].squeeze().cpu().numpy()\n",
    "\n",
    "                pred_map_np[y:y+patch_size, x:x+patch_size] += pred_np_patch\n",
    "                pred_map_hv[:, y:y+patch_size, x:x+patch_size] += pred_hv_patch\n",
    "                pred_map_tp[:, y:y+patch_size, x:x+patch_size] += pred_tp_patch\n",
    "                count_map[y:y+patch_size, x:x+patch_size] += 1.0\n",
    "\n",
    "    count_map[count_map == 0] = 1 # Avoid division by zero\n",
    "    final_pred_np = pred_map_np / count_map\n",
    "    final_pred_hv = pred_map_hv / count_map\n",
    "    final_pred_tp = pred_map_tp / count_map\n",
    "\n",
    "    final_pred_np = final_pred_np[0:H, 0:W]\n",
    "    final_pred_hv = final_pred_hv[:, 0:H, 0:W]\n",
    "    final_pred_tp = final_pred_tp[:, 0:H, 0:W]\n",
    "\n",
    "    return final_pred_np, final_pred_hv, final_pred_tp\n",
    "\n",
    "def get_pq(pred_mask, gt_mask, iou_thresh=0.5):\n",
    "    \"\"\"Calculates Panoptic Quality for a single class.\"\"\"\n",
    "    pred_labels = np.unique(pred_mask)[1:] # Ignore 0\n",
    "    gt_labels = np.unique(gt_mask)[1:] # Ignore 0\n",
    "\n",
    "    if len(pred_labels) == 0 and len(gt_labels) == 0:\n",
    "        return 1.0, 0, 0, 0, 0 # Both empty, perfect score\n",
    "    elif len(pred_labels) == 0 or len(gt_labels) == 0:\n",
    "        return 0.0, 0, 0, len(pred_labels), len(gt_labels) # One empty\n",
    "\n",
    "    iou_matrix = np.zeros((len(pred_labels), len(gt_labels)), dtype=np.float32)\n",
    "    for i, pred_id in enumerate(pred_labels):\n",
    "        pred_instance = (pred_mask == pred_id)\n",
    "        for j, gt_id in enumerate(gt_labels):\n",
    "            gt_instance = (gt_mask == gt_id)\n",
    "\n",
    "            intersection = np.logical_and(pred_instance, gt_instance).sum()\n",
    "            union = np.logical_or(pred_instance, gt_instance).sum()\n",
    "\n",
    "            if union > 0:\n",
    "                iou_matrix[i, j] = intersection / union\n",
    "\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "\n",
    "    tp = 0\n",
    "    iou_sum = 0.0\n",
    "    matched_pred_indices = set()\n",
    "    matched_gt_indices = set()\n",
    "    for pred_idx, gt_idx in zip(row_ind, col_ind):\n",
    "        if iou_matrix[pred_idx, gt_idx] >= iou_thresh:\n",
    "            tp += 1\n",
    "            iou_sum += iou_matrix[pred_idx, gt_idx]\n",
    "            matched_pred_indices.add(pred_idx)\n",
    "            matched_gt_indices.add(gt_idx)\n",
    "\n",
    "    fp = len(pred_labels) - len(matched_pred_indices)\n",
    "    fn = len(gt_labels) - len(matched_gt_indices)\n",
    "\n",
    "    pq = iou_sum / (tp + 0.5 * fp + 0.5 * fn + 1e-8)\n",
    "\n",
    "    return pq, iou_sum, tp, fp, fn\n",
    "\n",
    "def compute_pq_for_image(pred_masks_by_class, gt_masks_by_class):\n",
    "    \"\"\"Computes the final wPQ score for a single image.\"\"\"\n",
    "    pq_scores = {}\n",
    "    total_pq = 0.0\n",
    "    total_weight = 0.0\n",
    "\n",
    "    if gt_masks_by_class:\n",
    "        ref_shape = list(gt_masks_by_class.values())[0].shape\n",
    "    elif pred_masks_by_class:\n",
    "        ref_shape = list(pred_masks_by_class.values())[0].shape\n",
    "    else:\n",
    "        ref_shape = (1,1) # Should not happen\n",
    "\n",
    "    for class_name, class_weight in CLASS_WEIGHTS.items():\n",
    "        pred_mask = pred_masks_by_class.get(class_name, np.zeros(ref_shape, dtype=np.uint16))\n",
    "        gt_mask = gt_masks_by_class.get(class_name, np.zeros(ref_shape, dtype=np.uint16))\n",
    "\n",
    "        pq, iou_sum, tp, fp, fn = get_pq(pred_mask, gt_mask)\n",
    "        pq_scores[class_name] = pq\n",
    "        total_pq += pq * class_weight\n",
    "        total_weight += class_weight\n",
    "\n",
    "    wpq = total_pq / (total_weight + 1e-8) # Add epsilon to denominator\n",
    "    return wpq, pq_scores\n",
    "\n",
    "print(f\"Loading best model from {MODEL_SAVE_PATH}...\")\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"Error: Model file not found at {MODEL_SAVE_PATH}. Please run training first.\")\n",
    "elif 'val_ids' not in locals() or not val_ids:\n",
    "    print(\"Error: 'val_ids' not found. Please run the stratified split cell first.\")\n",
    "elif 'HoverNet' not in locals():\n",
    "    print(\"Error: 'HoverNet' class is not defined. Please run the model definition cell first.\")\n",
    "else:\n",
    "    print(\"Finding 'fast' validation images...\")\n",
    "    PIXEL_THRESHOLD = 500000\n",
    "    val_ids_fast = []\n",
    "    for image_id in val_ids:\n",
    "        img_path = os.path.join(DATA_DIR, f\"{image_id}.tif\")\n",
    "        try:\n",
    "            with tifffile.TiffFile(img_path) as tif:\n",
    "                shape = tif.pages[0].shape\n",
    "            num_pixels = shape[0] * shape[1]\n",
    "            if num_pixels < PIXEL_THRESHOLD:\n",
    "                val_ids_fast.append(image_id)\n",
    "        except Exception as e:\n",
    "            val_ids_fast.append(image_id) # Add it just in case\n",
    "\n",
    "    print(f\"Running 2D Grid Search on {len(val_ids_fast)} fast images...\")\n",
    "\n",
    "    model_for_eval = HoverNet(num_classes=TP_MAP_CHANNELS).to(DEVICE)\n",
    "    model_for_eval.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    model_for_eval.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    thresholds_to_test = [0.5, 0.6, 0.7, 0.8]\n",
    "    dists_to_test = [5, 7, 9]\n",
    "\n",
    "    best_wpq = -1.0\n",
    "    best_thresh = -1.0\n",
    "    best_dist = -1.0\n",
    "\n",
    "    for dist in dists_to_test:\n",
    "        for thresh in thresholds_to_test:\n",
    "            print(f\"\\n--- Testing combo: (np_thresh = {thresh}, min_dist = {dist}) ---\")\n",
    "\n",
    "            all_wpq_scores = []\n",
    "            pbar = tqdm(val_ids_fast, desc=f\"Testing (t={thresh}, d={dist})\")\n",
    "\n",
    "            for image_id in pbar:\n",
    "                try:\n",
    "                    img_path = os.path.join(DATA_DIR, f\"{image_id}.tif\")\n",
    "                    image = tifffile.imread(img_path)\n",
    "                    if image.ndim == 3 and image.shape[-1] == 4: image = image[:, :, :3]\n",
    "                    if image.ndim == 2: image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "                    if image.dtype != np.uint8:\n",
    "                        if image.max() > 255: image = (image / image.max() * 255).astype(np.uint8)\n",
    "                        else: image = image.astype(np.uint8)\n",
    "                    H, W, _ = image.shape\n",
    "\n",
    "                    pred_np, pred_hv, pred_tp = sliding_window_inference(\n",
    "                        model_for_eval, image, PATCH_SIZE, OVERLAP, device\n",
    "                    )\n",
    "\n",
    "                    pred_masks_by_class = post_process(\n",
    "                        pred_np, pred_hv, pred_tp,\n",
    "                        np_thresh=thresh,\n",
    "                        marker_min_dist=dist\n",
    "                    )\n",
    "\n",
    "                    mask_path = os.path.join(MASK_DIR, f\"{image_id}.npz\")\n",
    "                    with np.load(mask_path) as data:\n",
    "                        gt_instance_map = data['instance_map']\n",
    "                        gt_type_map = data['type_map']\n",
    "                    gt_masks_by_class = {}\n",
    "                    for class_id, class_name in CLASS_MAP.items():\n",
    "                        class_instances = np.unique(gt_instance_map[gt_type_map == class_id])\n",
    "                        class_instances = class_instances[class_instances != 0]\n",
    "                        gt_mask = np.zeros((H, W), dtype=np.uint16)\n",
    "                        for new_id, inst_id in enumerate(class_instances, 1):\n",
    "                            gt_mask[gt_instance_map == inst_id] = new_id\n",
    "                        gt_masks_by_class[class_name] = gt_mask\n",
    "\n",
    "                    wpq, _ = compute_pq_for_image(pred_masks_by_class, gt_masks_by_class)\n",
    "                    all_wpq_scores.append(wpq)\n",
    "                    pbar.set_postfix(avg_wPQ=f\"{np.mean(all_wpq_scores):.4f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR on {image_id}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if all_wpq_scores:\n",
    "                avg_wpq = np.mean(all_wpq_scores)\n",
    "            else:\n",
    "                avg_wpq = 0.0 # Assign 0 if all images failed\n",
    "\n",
    "            print(f\"  Result for (t={thresh}, d={dist}): avg_wPQ = {avg_wpq:.4f}\")\n",
    "\n",
    "            if avg_wpq > best_wpq:\n",
    "                best_wpq = avg_wpq\n",
    "                best_thresh = thresh\n",
    "                best_dist = dist\n",
    "\n",
    "    print(\"\\n--- 2D Grid Search Complete ---\")\n",
    "    print(f\"Best wPQ Score: {best_wpq:.4f}\")\n",
    "    print(f\"Found at: np_thresh = {best_thresh}, marker_min_dist = {best_dist}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "te02pNCJzmMf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wj3pyNVolWx9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "state": {},
   "version_major": 2,
   "version_minor": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
